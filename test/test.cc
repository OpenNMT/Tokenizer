#include <gtest/gtest.h>

#include <onmt/BPE.h>
#include <onmt/SentencePiece.h>
#include <onmt/Tokenizer.h>
#include <onmt/SpaceTokenizer.h>

using namespace onmt;

static std::string data_dir;

static std::string get_data(const std::string& path) {
  return data_dir + "/" + path;
}

static void test_tok(ITokenizer& tokenizer,
                     const std::string& in,
                     const std::string& expected,
                     bool detokenize = false,
                     bool training = true) {
  std::vector<std::string> tokens;
  std::vector<std::vector<std::string>> features;
  tokenizer.tokenize(in, tokens, features, training);
  EXPECT_EQ(SpaceTokenizer::get_instance().detokenize(tokens, features), expected);
  if (detokenize) {
    EXPECT_EQ(tokenizer.detokenize(tokens, features), in);
  }
}

static void test_tok(ITokenizer& tokenizer,
                     const std::string& in,
                     const std::vector<std::string>& expected,
                     bool detokenize = false) {
  std::vector<std::string> tokens;
  tokenizer.tokenize(in, tokens);
  ASSERT_EQ(tokens.size(), expected.size());
  for (size_t i = 0; i < tokens.size(); ++i) {
    EXPECT_EQ(tokens[i], expected[i])  << "Unexpected token mismatch at index " << i;
  }
  if (detokenize) {
    auto text = tokenizer.detokenize(tokens);
    EXPECT_EQ(text, in);
  }
}

static void test_detok(ITokenizer& tokenizer, const std::string& in, const std::string& expected) {
  std::vector<std::string> tokens;
  onmt::SpaceTokenizer::get_instance().tokenize(in, tokens);
  auto detok = tokenizer.detokenize(tokens);
  EXPECT_EQ(detok, expected);
}

static void test_tok_alphabet(ITokenizer& tokenizer,
                              const std::string& in,
                              const std::string& expected,
                              const std::unordered_map<std::string, size_t>& expected_alphabets) {
  std::vector<std::string> words;
  std::vector<std::vector<std::string> > features;
  std::unordered_map<std::string, size_t> alphabets;

  tokenizer.tokenize(in, words, features, alphabets);

  std::string output;
  for (size_t i = 0; i < words.size(); ++i)
  {
    if (i > 0)
      output += " ";
    output += words[i];
  }

  EXPECT_EQ(output, expected);

  for (const auto& it: expected_alphabets) {
    ASSERT_TRUE(alphabets.find(it.first) != alphabets.end());
    EXPECT_EQ(alphabets[it.first], it.second);
  }
}

static void test_tok_and_detok(ITokenizer& tokenizer,
                               const std::string& in,
                               const std::string& expected) {
  return test_tok(tokenizer, in, expected, true);
}

TEST(TokenizerTest, DetokenizeWithRanges) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative);
  Ranges ranges;
  tokenizer.detokenize({"HelÔø≠", "lo", "ÔΩümrk_case_modifier_CÔΩ†", "w", "Ôø≠", "orld", "Ôø≠!"}, ranges);
  // Result: Hello World!
  ASSERT_EQ(ranges.size(), 5);
  EXPECT_EQ(ranges[0], (std::pair<size_t, size_t>(0, 2)));
  EXPECT_EQ(ranges[1], (std::pair<size_t, size_t>(3, 4)));
  EXPECT_EQ(ranges[3], (std::pair<size_t, size_t>(6, 6)));
  EXPECT_EQ(ranges[5], (std::pair<size_t, size_t>(7, 10)));
  EXPECT_EQ(ranges[6], (std::pair<size_t, size_t>(11, 11)));
}

TEST(TokenizerTest, DetokenizeWithMergedRanges) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative);
  Ranges ranges;
  tokenizer.detokenize({"HelÔø≠", "lo", "wÔø≠", "orld", "Ôø≠!"}, ranges, true);
  // Result: Hello World!
  ASSERT_EQ(ranges.size(), 5);
  EXPECT_EQ(ranges[0], (std::pair<size_t, size_t>(0, 4)));
  EXPECT_EQ(ranges[1], (std::pair<size_t, size_t>(0, 4)));
  EXPECT_EQ(ranges[2], (std::pair<size_t, size_t>(6, 10)));
  EXPECT_EQ(ranges[3], (std::pair<size_t, size_t>(6, 10)));
  EXPECT_EQ(ranges[4], (std::pair<size_t, size_t>(11, 11)));
}

TEST(TokenizerTest, DetokenizeWithMergedRangesPlaceholders) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative);
  Ranges ranges;
  tokenizer.detokenize({"ÔΩüaÔΩ†Ôø≠", "b", "Ôø≠ÔΩücÔΩ†"}, ranges, true);
  // Result: ÔΩüaÔΩ†bÔΩücÔΩ†
  ASSERT_EQ(ranges.size(), 3);
  EXPECT_EQ(ranges[0], (std::pair<size_t, size_t>(0, 6)));
  EXPECT_EQ(ranges[1], (std::pair<size_t, size_t>(7, 7)));
  EXPECT_EQ(ranges[2], (std::pair<size_t, size_t>(8, 14)));
}

TEST(TokenizerTest, BasicConservative) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative);
  test_tok(tokenizer,
           "Your Hardware-Enablement Stack (HWE) is supported until April 2019.",
           "Your Hardware-Enablement Stack ( HWE ) is supported until April 2019 .");
}
TEST(TokenizerTest, ConservativeEmpty) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative);
  test_tok(tokenizer, "", "");
}

TEST(TokenizerTest, None) {
  Tokenizer tokenizer(Tokenizer::Mode::None);
  test_tok(tokenizer, "Hello World!", "Hello World!");
}

TEST(TokenizerTest, NoneWithPlaceholders1) {
  Tokenizer tokenizer(Tokenizer::Mode::None, Tokenizer::Flags::JoinerAnnotate);
  test_tok(tokenizer, "Hello:ÔΩüWorldÔΩ†!", "Hello:Ôø≠ ÔΩüWorldÔΩ†Ôø≠ !");
}

TEST(TokenizerTest, NoneWithPlaceholders2) {
  Tokenizer tokenizer(Tokenizer::Mode::None, Tokenizer::Flags::JoinerAnnotate |
                      Tokenizer::Flags::PreservePlaceholders);
  test_tok(tokenizer, "Hello:ÔΩüWorldÔΩ†!", "Hello:Ôø≠ ÔΩüWorldÔΩ† Ôø≠ !");
}

TEST(TokenizerTest, NonePlaceholderSpacesEscape) {
  Tokenizer tokenizer(Tokenizer::Mode::None, Tokenizer::Flags::None);
  test_tok(tokenizer, "ÔΩüa b cÔΩ†", "ÔΩüaÔºÖ0020bÔºÖ0020cÔΩ†");
}

TEST(TokenizerTest, NonePlaceholderSpacesNoEscape) {
  Tokenizer tokenizer(Tokenizer::Mode::None, Tokenizer::Flags::NoSubstitution);
  test_tok(tokenizer, "ÔΩüa b cÔΩ†", "ÔΩüa b cÔΩ†");
}

TEST(TokenizerTest, PreserveTokensInNoneMode) {
  Tokenizer tokenizer(Tokenizer::Mode::None,
                      Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::PreserveSegmentedTokens);
  test_tok(tokenizer, "HelloÔΩüWorldÔΩ†!", "Hello Ôø≠ ÔΩüWorldÔΩ† Ôø≠ !");
}

TEST(TokenizerTest, BasicSpace) {
  Tokenizer tokenizer(Tokenizer::Mode::Space);
  test_tok(tokenizer,
           "49th meeting Social and human rights questions: human rights [14 (g)]",
           "49th meeting Social and human rights questions: human rights [14 (g)]");
}
TEST(TokenizerTest, SpaceEmpty) {
  Tokenizer tokenizer(Tokenizer::Mode::Space);
  test_tok(tokenizer, "", "");
}
TEST(TokenizerTest, SpaceSingle) {
  Tokenizer tokenizer(Tokenizer::Mode::Space);
  test_tok(tokenizer, "Hello", "Hello");
}
TEST(TokenizerTest, SpaceLeading) {
  Tokenizer tokenizer(Tokenizer::Mode::Space);
  test_tok(tokenizer, " Hello", "Hello");
}
TEST(TokenizerTest, SpaceTrailing) {
  Tokenizer tokenizer(Tokenizer::Mode::Space);
  test_tok(tokenizer, "Hello ", "Hello");
}
TEST(TokenizerTest, SpaceDuplicated) {
  Tokenizer tokenizer(Tokenizer::Mode::Space);
  test_tok(tokenizer, "  Hello   World ", "Hello World");
}

TEST(TokenizerTest, SpacePlaceholderSpacesEscape) {
  Tokenizer tokenizer(Tokenizer::Mode::Space, Tokenizer::Flags::JoinerAnnotate);
  test_tok(tokenizer, "aÔΩüb cÔΩ† d", "aÔø≠ ÔΩübÔºÖ0020cÔΩ† d");
}

TEST(TokenizerTest, BasicJoiner) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::JoinerAnnotate);
  test_tok_and_detok(tokenizer,
                     "Isn't it so-greatly working?",
                     "Isn Ôø≠'Ôø≠ t it so Ôø≠-Ôø≠ greatly working Ôø≠?");
  test_tok_and_detok(tokenizer, "MP3", "MP Ôø≠3");
  test_tok_and_detok(tokenizer, "A380", "A Ôø≠380");
  test_tok_and_detok(tokenizer, "$1", "$Ôø≠ 1");
}

TEST(TokenizerTest, BasicSpaceWithFeatures) {
  Tokenizer tokenizer(Tokenizer::Mode::Space, Tokenizer::Flags::CaseFeature);
  std::vector<std::string> tokens;
  std::vector<std::vector<std::string>> features;
  tokenizer.tokenize("HelloÔø®12Ôø®AB worldÔø®34Ôø®CD", tokens, features);
  EXPECT_EQ(tokens, (std::vector<std::string>{"hello", "world"}));
  ASSERT_EQ(features.size(), 3);
  EXPECT_EQ(features[0], (std::vector<std::string>{"12", "34"}));
  EXPECT_EQ(features[1], (std::vector<std::string>{"AB", "CD"}));
  EXPECT_EQ(features[2], (std::vector<std::string>{"C", "L"}));
}

TEST(TokenizerTest, ProtectedSequence) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::JoinerAnnotate);
  test_tok_and_detok(tokenizer, "ÔΩü1,023ÔΩ†km", "ÔΩü1,023ÔΩ†Ôø≠ km");
  test_tok_and_detok(tokenizer, "AÔΩü380ÔΩ†", "A Ôø≠ÔΩü380ÔΩ†");
  test_tok_and_detok(tokenizer, "ÔΩü1,023ÔΩ†ÔΩü380ÔΩ†", "ÔΩü1,023ÔΩ†Ôø≠ ÔΩü380ÔΩ†");
  test_tok_and_detok(tokenizer, "ÔΩü1023ÔΩ†.", "ÔΩü1023ÔΩ† Ôø≠.");
  test_tok_and_detok(tokenizer, "$ÔΩü0.23ÔΩ†", "$Ôø≠ ÔΩü0.23ÔΩ†");
  test_tok_and_detok(tokenizer, "ÔΩü0.23ÔΩ†$", "ÔΩü0.23ÔΩ† Ôø≠$");
  test_tok_and_detok(tokenizer, "ÔΩüUS$ÔΩ†23", "ÔΩüUS$ÔΩ†Ôø≠ 23");
  test_tok_and_detok(tokenizer, "1ÔΩüABCDÔΩ†0", "1 Ôø≠ÔΩüABCDÔΩ†Ôø≠ 0");
}

TEST(TokenizerTest, PreserveProtectedSequence) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::PreservePlaceholders);
  test_tok_and_detok(tokenizer, "ÔΩü1,023ÔΩ†km", "ÔΩü1,023ÔΩ† Ôø≠ km");
  test_tok_and_detok(tokenizer, "AÔΩü380ÔΩ†", "A Ôø≠ ÔΩü380ÔΩ†");
  test_tok_and_detok(tokenizer, "ÔΩü1,023ÔΩ†ÔΩü380ÔΩ†", "ÔΩü1,023ÔΩ† Ôø≠ ÔΩü380ÔΩ†");
  test_tok_and_detok(tokenizer, "ÔΩü1023ÔΩ†.", "ÔΩü1023ÔΩ† Ôø≠.");
  test_tok_and_detok(tokenizer, "$ÔΩü0.23ÔΩ†", "$Ôø≠ ÔΩü0.23ÔΩ†");
  test_tok_and_detok(tokenizer, "ÔΩü0.23ÔΩ†$", "ÔΩü0.23ÔΩ† Ôø≠$");
  test_tok_and_detok(tokenizer, "ÔΩüUS$ÔΩ†23", "ÔΩüUS$ÔΩ† Ôø≠ 23");
  test_tok_and_detok(tokenizer, "1ÔΩüABCDÔΩ†0", "1 Ôø≠ ÔΩüABCDÔΩ† Ôø≠ 0");
}

TEST(TokenizerTest, PreserveProtectedSequenceSpacerAnnotate) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::SpacerAnnotate | Tokenizer::Flags::PreservePlaceholders);
  test_tok_and_detok(tokenizer, "ÔΩü1,023ÔΩ†km", "ÔΩü1,023ÔΩ† km");
  test_tok_and_detok(tokenizer, "A ÔΩü380ÔΩ†", "A ‚ñÅ ÔΩü380ÔΩ†");
  test_tok_and_detok(tokenizer, "ÔΩü1,023ÔΩ†ÔΩü380ÔΩ†", "ÔΩü1,023ÔΩ† ÔΩü380ÔΩ†");
  test_tok_and_detok(tokenizer, "ÔΩü1,023ÔΩ† ÔΩü380ÔΩ†", "ÔΩü1,023ÔΩ† ‚ñÅ ÔΩü380ÔΩ†");
  test_tok_and_detok(tokenizer, "ÔΩü1023ÔΩ†.", "ÔΩü1023ÔΩ† .");
  test_tok_and_detok(tokenizer, "ÔΩü1023ÔΩ† .", "ÔΩü1023ÔΩ† ‚ñÅ.");
}

TEST(TokenizerTest, ProtectedSequenceAggressive) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::JoinerAnnotate);
  test_tok_and_detok(tokenizer, "ÔΩü1,023ÔΩ†km", "ÔΩü1,023ÔΩ†Ôø≠ km");
  test_tok_and_detok(tokenizer, "AÔΩü380ÔΩ†", "A Ôø≠ÔΩü380ÔΩ†");
  test_tok_and_detok(tokenizer, "ÔΩü1,023ÔΩ†ÔΩü380ÔΩ†", "ÔΩü1,023ÔΩ†Ôø≠ ÔΩü380ÔΩ†");
  test_tok_and_detok(tokenizer, "ÔΩü1023ÔΩ†.", "ÔΩü1023ÔΩ† Ôø≠.");
  test_tok_and_detok(tokenizer, "$ÔΩü0.23ÔΩ†", "$Ôø≠ ÔΩü0.23ÔΩ†");
  test_tok_and_detok(tokenizer, "ÔΩü0.23ÔΩ†$", "ÔΩü0.23ÔΩ† Ôø≠$");
  test_tok_and_detok(tokenizer, "ÔΩüUS$ÔΩ†23", "ÔΩüUS$ÔΩ†Ôø≠ 23");
  test_tok_and_detok(tokenizer, "1ÔΩüABCDÔΩ†0", "1 Ôø≠ÔΩüABCDÔΩ†Ôø≠ 0");
}

TEST(TokenizerTest, ProtectedSequenceJoinerNew) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::JoinerNew);
  test_tok_and_detok(tokenizer, "ÔΩü1,023ÔΩ†km", "ÔΩü1,023ÔΩ† Ôø≠ km");
  test_tok_and_detok(tokenizer, "AÔΩü380ÔΩ†", "A Ôø≠ ÔΩü380ÔΩ†");
  test_tok_and_detok(tokenizer, "ÔΩü1,023ÔΩ†ÔΩü380ÔΩ†", "ÔΩü1,023ÔΩ† Ôø≠ ÔΩü380ÔΩ†");
  test_tok_and_detok(tokenizer, "ÔΩü1023ÔΩ†.", "ÔΩü1023ÔΩ† Ôø≠ .");
}

TEST(TokenizerTest, Substitutes) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative);
  test_tok(tokenizer,
           "testÔø≠ protectÔø®, Ôºö, ‚ñÅ, and ÔºÖ or ÔºÉ...",
           "test ‚ñ† protect ‚îÇ , : , _ , and % or # . . .");
  test_tok(tokenizer, "ÔΩütagÔºövalue with spacesÔΩ†", "ÔΩütagÔºövalueÔºÖ0020withÔºÖ0020spacesÔΩ†");
}

TEST(TokenizerTest, NoSubstitution) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::NoSubstitution);
  test_tok(tokenizer,
           "testÔø≠ protectÔø®, Ôºö, ‚ñÅ, and ÔºÖ or ÔºÉ...",
           "test Ôø≠ protect Ôø® , Ôºö , ‚ñÅ , and ÔºÖ or ÔºÉ . . .");
  test_tok(tokenizer, "ÔΩütagÔºövalue with spacesÔΩ†", "ÔΩütagÔºövalue with spacesÔΩ†");
}

TEST(TokenizerTest, ZeroWidthJoiner) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::JoinerAnnotate);
  test_tok_and_detok(tokenizer, "üë®‚Äçüë©‚Äçüë¶", "üë® Ôø≠‚Äç Ôø≠üë© Ôø≠‚Äç Ôø≠üë¶");
}

TEST(TokenizerTest, CombiningMark) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::JoinerAnnotate);
  test_tok_and_detok(tokenizer,
                     "‡§µ‡§∞‡•ç‡§§‡§Æ‡§æ‡§® ‡§≤‡§ø‡§™‡§ø (‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü) ‡§ñ‡•ã ‡§ú‡§æ‡§è‡§ó‡•Ä‡•§",
                     "‡§µ‡§∞‡•ç‡§§‡§Æ‡§æ‡§® ‡§≤‡§ø‡§™‡§ø (Ôø≠ ‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü Ôø≠) ‡§ñ‡•ã ‡§ú‡§æ‡§è‡§ó‡•Ä Ôø≠‡•§");
}

TEST(TokenizerTest, MarkOnSpace) {
  Tokenizer tokenizer_joiner(Tokenizer::Mode::Conservative, Tokenizer::Flags::JoinerAnnotate);
  test_tok_and_detok(tokenizer_joiner,
                     "b Ãác",
                     "b Ôø≠ÔºÖ0020ÃáÔø≠ c");
  Tokenizer tokenizer_spacer(Tokenizer::Mode::Conservative, Tokenizer::Flags::SpacerAnnotate);
  test_tok_and_detok(tokenizer_spacer,
                     "b Ãác",
                     "b ÔºÖ0020Ãá c");
}

TEST(TokenizerTest, MarkOnSpaceNoSubstitution) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::NoSubstitution);
  test_tok(tokenizer, "angles ·Åß1 and ·Åß2", {"angles", "Ôø≠ ·ÅßÔø≠", "1", "and", "Ôø≠ ·ÅßÔø≠", "2"}, true);
}

TEST(TokenizerTest, CombiningMarkAfterPlaceholder) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::PreservePlaceholders);
  test_tok_and_detok(tokenizer, "ÔΩüaÔΩ†◊Çb", "ÔΩüaÔΩ† Ôø≠◊ÇÔø≠ b");
}

TEST(TokenizerTest, CaseFeature) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::CaseFeature | Tokenizer::Flags::JoinerAnnotate);
  // Note: in C literal strings, \ is escaped by another \.
  test_tok(tokenizer,
           "test \\\\\\\\a Capitalized lowercased UPPERCAS√â miX√™d - cyrillic-–ë",
           "testÔø®L \\Ôø®N Ôø≠\\Ôø®N Ôø≠\\Ôø®N Ôø≠\\Ôø≠Ôø®N aÔø®L capitalizedÔø®C lowercasedÔø®L uppercas√©Ôø®U mix√™dÔø®M -Ôø®N cyrillic-–±Ôø®M");
}

TEST(TokenizerTest, CaseFeatureWithJoinerNew) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      Tokenizer::Flags::CaseFeature
                      | Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::JoinerNew);
  test_tok(tokenizer, "a-b.", "aÔø®L Ôø≠Ôø®N -Ôø®N Ôø≠Ôø®N bÔø®L Ôø≠Ôø®N .Ôø®N");
}

TEST(TokenizerTest, CaseFeatureWithSpacerNew) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::CaseFeature
                      | Tokenizer::Flags::SpacerAnnotate
                      | Tokenizer::Flags::SpacerNew);
  test_tok(tokenizer, "a b", "aÔø®L ‚ñÅÔø®N bÔø®L");
}

TEST(TokenizerTest, CaseMarkupWithJoiners) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::CaseMarkup | Tokenizer::Flags::JoinerAnnotate);
  test_tok_and_detok(tokenizer,
                     "Hello world!", "ÔΩümrk_case_modifier_CÔΩ† hello world Ôø≠!");
  test_tok_and_detok(tokenizer,
                     "Hello WORLD!", "ÔΩümrk_case_modifier_CÔΩ† hello ÔΩümrk_begin_case_region_UÔΩ† world ÔΩümrk_end_case_region_UÔΩ† Ôø≠!");
  test_tok_and_detok(tokenizer,
                     "HELLO WORLD!", "ÔΩümrk_begin_case_region_UÔΩ† hello ÔΩümrk_end_case_region_UÔΩ† ÔΩümrk_begin_case_region_UÔΩ† world ÔΩümrk_end_case_region_UÔΩ† Ôø≠!");
  test_tok_and_detok(tokenizer,
                     "Hello WOrld!", "ÔΩümrk_case_modifier_CÔΩ† hello ÔΩümrk_begin_case_region_UÔΩ† woÔø≠ ÔΩümrk_end_case_region_UÔΩ† rld Ôø≠!");
  test_tok_and_detok(tokenizer,
                     "hello woRld!", "hello woÔø≠ ÔΩümrk_case_modifier_CÔΩ† rld Ôø≠!");
  test_tok_and_detok(tokenizer,
                     "hello woRlD!", "hello woÔø≠ ÔΩümrk_case_modifier_CÔΩ† rlÔø≠ ÔΩümrk_case_modifier_CÔΩ† d Ôø≠!");
}

TEST(TokenizerTest, CaseMarkupWithSoftUppercaseRegions) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      Tokenizer::Flags::CaseMarkup
                      | Tokenizer::Flags::SoftCaseRegions
                      | Tokenizer::Flags::JoinerAnnotate);
  test_tok_and_detok(tokenizer,
                     "AA.BB", "ÔΩümrk_begin_case_region_UÔΩ† aa Ôø≠.Ôø≠ bb ÔΩümrk_end_case_region_UÔΩ†");
  test_tok_and_detok(tokenizer,
                     "A BC", "ÔΩümrk_begin_case_region_UÔΩ† a bc ÔΩümrk_end_case_region_UÔΩ†");
  test_tok_and_detok(tokenizer,
                     "AA.", "ÔΩümrk_begin_case_region_UÔΩ† aa ÔΩümrk_end_case_region_UÔΩ† Ôø≠.");
  test_tok_and_detok(tokenizer,
                     "A-B/C", "ÔΩümrk_begin_case_region_UÔΩ† a Ôø≠-Ôø≠ b Ôø≠/Ôø≠ c ÔΩümrk_end_case_region_UÔΩ†");
  test_tok_and_detok(tokenizer,
                     "A-B/c", "ÔΩümrk_begin_case_region_UÔΩ† a Ôø≠-Ôø≠ b ÔΩümrk_end_case_region_UÔΩ† Ôø≠/Ôø≠ c");
  test_tok_and_detok(tokenizer,
                     "A", "ÔΩümrk_case_modifier_CÔΩ† a");
  test_tok_and_detok(tokenizer,
                     "A-", "ÔΩümrk_case_modifier_CÔΩ† a Ôø≠-");
  test_tok_and_detok(tokenizer,
                     "ID: A23X52,",
                     "ÔΩümrk_begin_case_region_UÔΩ† id Ôø≠: a Ôø≠23Ôø≠ x Ôø≠52 ÔΩümrk_end_case_region_UÔΩ† Ôø≠,");
  test_tok_and_detok(tokenizer,
                     "Show PP-LX-DP",
                     "ÔΩümrk_case_modifier_CÔΩ† show ÔΩümrk_begin_case_region_UÔΩ† pp Ôø≠-Ôø≠ lx Ôø≠-Ôø≠ dp ÔΩümrk_end_case_region_UÔΩ†");
  test_tok_and_detok(tokenizer,
                     "AA ÔΩüBBÔΩ† CC", "ÔΩümrk_begin_case_region_UÔΩ† aa ÔΩümrk_end_case_region_UÔΩ† ÔΩüBBÔΩ† ÔΩümrk_begin_case_region_UÔΩ† cc ÔΩümrk_end_case_region_UÔΩ†");
}

TEST(TokenizerTest, CaseMarkupWithJoinerNew) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::CaseMarkup
                      | Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::JoinerNew);
  test_detok(tokenizer, "hello ÔΩümrk_case_modifier_CÔΩ† Ôø≠ world !", "helloWorld !");
  test_detok(tokenizer, "hello ÔΩümrk_case_modifier_CÔΩ† Ôø≠", "hello");
}

TEST(TokenizerTest, CaseMarkupWithSpacers) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::CaseMarkup | Tokenizer::Flags::SpacerAnnotate);
  test_tok_and_detok(tokenizer,
                     "Hello world!", "ÔΩümrk_case_modifier_CÔΩ† hello ‚ñÅworld !");
  test_tok_and_detok(tokenizer,
                     "Hello WORLD!", "ÔΩümrk_case_modifier_CÔΩ† hello ÔΩümrk_begin_case_region_UÔΩ† ‚ñÅworld ÔΩümrk_end_case_region_UÔΩ† !");
  test_tok_and_detok(tokenizer,
                     "Hello WOrld!", "ÔΩümrk_case_modifier_CÔΩ† hello ÔΩümrk_begin_case_region_UÔΩ† ‚ñÅwo ÔΩümrk_end_case_region_UÔΩ† rld !");
  test_tok_and_detok(tokenizer,
                     "hello woRld!", "hello ‚ñÅwo ÔΩümrk_case_modifier_CÔΩ† rld !");
}

TEST(TokenizerTest, CaseMarkupWithSpacerNew) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::CaseMarkup
                      | Tokenizer::Flags::SpacerAnnotate
                      | Tokenizer::Flags::SpacerNew);
  test_detok(tokenizer, "hello ÔΩümrk_case_modifier_CÔΩ† ‚ñÅ world !", "hello World!");
  test_detok(tokenizer, "hello ÔΩümrk_case_modifier_CÔΩ† ‚ñÅ", "hello ");
}

TEST(TokenizerTest, CaseMarkupWithBPE) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::CaseMarkup | Tokenizer::Flags::JoinerAnnotate,
                      get_data("bpe-models/codes_suffix_case_insensitive.fr"));
  test_tok_and_detok(tokenizer,
                     "Bonjour monde", "ÔΩümrk_case_modifier_CÔΩ† bonÔø≠ jÔø≠ our monÔø≠ de");
  test_tok_and_detok(tokenizer,
                     "BONJOUR MONDE", "ÔΩümrk_begin_case_region_UÔΩ† bonÔø≠ jÔø≠ our ÔΩümrk_end_case_region_UÔΩ† ÔΩümrk_begin_case_region_UÔΩ† monÔø≠ de ÔΩümrk_end_case_region_UÔΩ†");
  test_tok_and_detok(tokenizer,
                     "BONJOUR monde", "ÔΩümrk_begin_case_region_UÔΩ† bonÔø≠ jÔø≠ our ÔΩümrk_end_case_region_UÔΩ† monÔø≠ de");
}

TEST(TokenizerTest, CaseMarkupDetokWithPlaceholders) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::CaseMarkup);
  test_detok(tokenizer, "ÔΩümrk_case_modifier_CÔΩ† ÔΩüabcÔΩ†", "ÔΩüabcÔΩ†");
  test_detok(tokenizer, "ÔΩümrk_begin_case_region_UÔΩ† ÔΩüabcÔΩ† ÔΩümrk_end_case_region_UÔΩ†", "ÔΩüabcÔΩ†");
}

TEST(TokenizerTest, CaseMarkupDetokMissingModifiedToken) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::CaseMarkup);
  test_detok(tokenizer, "hello ÔΩümrk_case_modifier_CÔΩ†", "hello");
  test_detok(tokenizer, "ÔΩümrk_case_modifier_CÔΩ† ÔΩümrk_case_modifier_CÔΩ† hello", "Hello");
  test_detok(tokenizer, "ÔΩümrk_case_modifier_CÔΩ† ÔΩümrk_begin_case_region_UÔΩ† hello ÔΩümrk_end_case_region_UÔΩ†", "HELLO");
}

TEST(TokenizerTest, CaseMarkupDetokMissingRegionMarker) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::CaseMarkup);
  test_detok(tokenizer, "ÔΩümrk_begin_case_region_UÔΩ† hello", "HELLO");
  test_detok(tokenizer,
             "ÔΩümrk_begin_case_region_UÔΩ† hello ÔΩümrk_case_modifier_CÔΩ† world", "HELLO World");
  test_detok(tokenizer,
             "ÔΩümrk_end_case_region_UÔΩ† hello ÔΩümrk_case_modifier_CÔΩ† world", "hello World");
}

TEST(TokenizerTest, CaseMarkupDetokNestedMarkers) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::CaseMarkup);
  test_detok(tokenizer,
             "ÔΩümrk_begin_case_region_UÔΩ† ÔΩümrk_case_modifier_CÔΩ† hello world ÔΩümrk_end_case_region_UÔΩ†", "Hello WORLD");
  test_detok(tokenizer,
             "ÔΩümrk_begin_case_region_UÔΩ† hello ÔΩümrk_case_modifier_CÔΩ† ÔΩümrk_end_case_region_UÔΩ† world", "HELLO world");
}

TEST(TokenizerTest, CaseMarkupWithLocaleEl) {
  Tokenizer::Options options;
  options.case_markup = true;
  options.soft_case_regions = true;
  options.lang = "el";
  Tokenizer tokenizer(options);
  test_tok(tokenizer,
           "Œ£ŒôŒìŒúŒë Œ§ŒïŒõŒôŒöŒüŒ£",
           "ÔΩümrk_begin_case_region_UÔΩ† œÉŒπŒ≥ŒºŒ± œÑŒµŒªŒπŒ∫ŒøœÇ ÔΩümrk_end_case_region_UÔΩ†");
  test_detok(tokenizer,
             "ÔΩümrk_begin_case_region_UÔΩ† œÑŒ∑ŒΩ Œ¨ŒΩŒøŒπŒæŒ∑ , Œ±œÄœÅŒØŒªŒπŒø ŒÆ ŒºŒ¨ŒπŒø , Œ∏Œ± Œ∫Œ±œÑŒ±ŒΩŒ±ŒªœéœÉœâ ŒºŒµŒ≥Œ±ŒªœçœÑŒµœÅŒµœÇ œÄŒøœÉœåœÑŒ∑œÑŒµœÇ œÄœÅœâœÑŒµŒêŒΩŒ∑œÇ ÔΩümrk_end_case_region_UÔΩ†",
             "Œ§ŒóŒù ŒëŒùŒüŒôŒûŒó , ŒëŒ†Œ°ŒôŒõŒôŒü ŒóÃÅ ŒúŒëŒ™Œü , ŒòŒë ŒöŒëŒ§ŒëŒùŒëŒõŒ©Œ£Œ© ŒúŒïŒìŒëŒõŒ•Œ§ŒïŒ°ŒïŒ£ Œ†ŒüŒ£ŒüŒ§ŒóŒ§ŒïŒ£ Œ†Œ°Œ©Œ§ŒïŒ™ŒùŒóŒ£");
}

TEST(TokenizerTest, CaseMarkupWithLocaleNl) {
  Tokenizer::Options options;
  options.case_markup = true;
  options.lang = "nl";
  Tokenizer tokenizer(options);
  test_detok(tokenizer, "ÔΩümrk_case_modifier_CÔΩ† ijssel", "IJssel");
}

TEST(TokenizerTest, SegmentCase) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::CaseFeature | Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::SegmentCase);
  test_tok_and_detok(tokenizer, "WiFi", "wiÔø≠Ôø®C fiÔø®C");
}

TEST(TokenizerTest, SegmentNumbers) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::SegmentNumbers);
  test_tok_and_detok(tokenizer,
                     "1984 mille neuf cent quatrevingt-quatre",
                     "1Ôø≠ 9Ôø≠ 8Ôø≠ 4 mille neuf cent quatrevingt Ôø≠-Ôø≠ quatre");
}

TEST(TokenizerTest, SegmentAlphabet) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::JoinerAnnotate);
  tokenizer.add_alphabet_to_segment("Han");
  test_tok_and_detok(tokenizer, "raw–ë", "raw–ë");
  test_tok(tokenizer,
           "ÊúâÂÖ•ËÅ≤ÂòÖÂîêË©±ÂæÄÂæÄÊúâÈôΩÂÖ•Â∞çËΩâÔºåÂç≥‰øÇÂÖ•ËÅ≤ÈüªÂ∞æÂêåÈºªÈü≥ÈüªÂ∞æÂèØ‰ª•ËΩâÂåñ„ÄÇÊØîÂ¶ÇÁ≤µË™ûÂòÖ„ÄåÊäå„ÄçÔºàdamÔºâ„ÄåÊèº„ÄçÔºàdapÔºâÔºåÊÑèÊÄùÊé•ËøëÔºåÊÑèÂë≥ÂæÆÂ¶ôÔºåÂçÄÂà•Âú®Êñº-mÂêå-pÂòÖËΩâÊèõ„ÄÇ",
           "ÊúâÔø≠ ÂÖ•Ôø≠ ËÅ≤Ôø≠ ÂòÖÔø≠ ÂîêÔø≠ Ë©±Ôø≠ ÂæÄÔø≠ ÂæÄÔø≠ ÊúâÔø≠ ÈôΩÔø≠ ÂÖ•Ôø≠ Â∞çÔø≠ ËΩâ Ôø≠ÔºåÔø≠ Âç≥Ôø≠ ‰øÇÔø≠ ÂÖ•Ôø≠ ËÅ≤Ôø≠ ÈüªÔø≠ Â∞æÔø≠ ÂêåÔø≠ ÈºªÔø≠ Èü≥Ôø≠ ÈüªÔø≠ Â∞æÔø≠ ÂèØÔø≠ ‰ª•Ôø≠ ËΩâÔø≠ Âåñ Ôø≠„ÄÇÔø≠ ÊØîÔø≠ Â¶ÇÔø≠ Á≤µÔø≠ Ë™ûÔø≠ ÂòÖ Ôø≠„ÄåÔø≠ Êäå Ôø≠„Äç Ôø≠ÔºàÔø≠ dam Ôø≠Ôºâ Ôø≠„ÄåÔø≠ Êèº Ôø≠„Äç Ôø≠ÔºàÔø≠ dap Ôø≠Ôºâ Ôø≠ÔºåÔø≠ ÊÑèÔø≠ ÊÄùÔø≠ Êé•Ôø≠ Ëøë Ôø≠ÔºåÔø≠ ÊÑèÔø≠ Âë≥Ôø≠ ÂæÆÔø≠ Â¶ô Ôø≠ÔºåÔø≠ ÂçÄÔø≠ Âà•Ôø≠ Âú®Ôø≠ Êñº-mÂêå-pÂòÖÔø≠ ËΩâÔø≠ Êèõ Ôø≠„ÄÇ");
}

// Checking backward compatibility with the "Kanbun" and "Kangxi" alphabets that are not
// included in ICU list of Unicode script aliases.
TEST(TokenizerTest, SegmentAlphabetKangxi) {
  Tokenizer::Options options;
  options.segment_alphabet = {"Kangxi"};
  Tokenizer tokenizer(options);
  test_tok(tokenizer, "12‚ºÄ‚ºÅ", "12 ‚ºÄ ‚ºÅ");
}
TEST(TokenizerTest, SegmentAlphabetKanbun) {
  Tokenizer::Options options;
  options.segment_alphabet = {"Kanbun"};
  Tokenizer tokenizer(options);
  test_tok(tokenizer, "12„Üô„Üö", "12 „Üô „Üö");
}

TEST(TokenizerTest, SegmentAlphabetChange) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::SegmentAlphabetChange);
  test_tok(tokenizer, "raw–ë", "raw –ë");
}

TEST(TokenizerTest, SegmentAlphabetChangeCommonScript) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::SegmentAlphabetChange);
  // Character „Éº can appear in both Hiragana and Katakana and should not be segmented when
  // appearing in these contexts. See https://github.com/OpenNMT/Tokenizer/issues/210.
  test_tok(tokenizer, "„Äå„Ç≠„É£„É≥„Éà„Éª„Éê„Ç§„Éª„Éü„Éº„Éª„É©„É¥„Äç", "„Äå „Ç≠„É£„É≥„Éà „Éª „Éê„Ç§ „Éª „Éü„Éº „Éª „É©„É¥ „Äç");
}

TEST(TokenizerTest, PreserveSegmentedNumbers) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      Tokenizer::Flags::SegmentNumbers
                      | Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::PreserveSegmentedTokens);
  test_tok_and_detok(tokenizer, "1234", "1 Ôø≠ 2 Ôø≠ 3 Ôø≠ 4");
}

TEST(TokenizerTest, PreserveSegmentAlphabetChange) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::SegmentAlphabetChange
                      | Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::PreserveSegmentedTokens);
  test_tok_and_detok(tokenizer, "raw–ë", "raw Ôø≠ –ë");
}

TEST(TokenizerTest, PreserveSegmentAlphabet) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::SegmentAlphabetChange
                      | Tokenizer::Flags::PreserveSegmentedTokens);
  tokenizer.add_alphabet_to_segment("Han");
  test_tok_and_detok(tokenizer, "Ê∏¨Ë©¶abc", "Ê∏¨ Ôø≠ Ë©¶ Ôø≠ abc");
}

TEST(TokenizerTest, PreserveSegmentCase) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::SegmentCase
                      | Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::PreserveSegmentedTokens);
  test_tok_and_detok(tokenizer, "WiFi", "Wi Ôø≠ Fi");
}

TEST(TokenizerTest, PreserveSegmentCaseBPE) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::SegmentCase
                      | Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::PreserveSegmentedTokens,
                      get_data("bpe-models/fr500"));
  test_tok_and_detok(tokenizer, "BonjourMonde", "BÔø≠ onÔø≠ jouÔø≠ r Ôø≠ MÔø≠ onÔø≠ de");
}

TEST(TokenizerTest, BPEBasic) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::JoinerAnnotate,
                      get_data("bpe-models/testcode.v0.1"));
  test_tok_and_detok(tokenizer,
                     "abcdimprovementËÅîÂêàÂõΩ",
                     "aÔø≠ bÔø≠ cÔø≠ dÔø≠ imprÔø≠ ovemenÔø≠ tÔø≠ ËÅîÂêàÔø≠ ÂõΩ");
}

TEST(TokenizerTest, BPEModePrefix) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::None,
                      get_data("bpe-models/codes_prefix.fr"));
  test_tok(tokenizer,
           "Seulement seulement il vais nonseulement seulementnon √† Verdun",
           "S e u lement seulement il v ais n on se u lement seulement n on √† V er d un");
}

TEST(TokenizerTest, BPEModeNofix) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::None,
                      get_data("bpe-models/codes_nofix.fr"));
  test_tok(tokenizer,
           "Seulement seulement il vais nonseulement seulementnon √† Verdun",
           "S e u lement seulement il v ais n on seulement seulement n on √† V er d un");
}

TEST(TokenizerTest, BPEModeBoth) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::None,
                      get_data("bpe-models/codes_bothfix.fr"));
  test_tok(tokenizer,
           "Seulement seulement il vais nonseulement seulementnon √† Verdun",
           "S eu lement seulement il va is n on s eu lement seu l emen t n on √† V er du n");
}

TEST(TokenizerTest, BPECaseInsensitive) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::None,
                      get_data("bpe-models/codes_suffix_case_insensitive.fr"));
  test_tok(tokenizer,
           "Seulement seulement il vais nonseulement seulementnon √† Verdun",
           "Seulement seulement il va is n on seulement seu l em ent n on √† Ver d un");
}

TEST(TokenizerTest, BPEDropout) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      new BPE(get_data("bpe-models/codes_suffix_case_insensitive.fr"), 1.0));
  test_tok(tokenizer, "seulement", "s e u l e m e n t");
  test_tok(tokenizer, "seulement", "seulement", /*detokenize=*/false, /*training=*/false);
}

TEST(TokenizerTest, BPEVocabularyWithTrailingJoiner) {
  BPE* bpe = new BPE(get_data("bpe-models/bpe_code.v0.2"));
  bpe->set_vocabulary(std::vector<std::string>{"welÔø≠"});
  Tokenizer tokenizer(Tokenizer::Mode::Space,
                      bpe,
                      Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::SupportPriorJoiners);
  test_tok(tokenizer, "welÔø≠ le", "welÔø≠ lÔø≠ e");
  test_tok(tokenizer, "wel le", "wÔø≠ eÔø≠ l lÔø≠ e");
}

TEST(TokenizerTest, BPEVocabularyWithLeadingJoiner) {
  BPE* bpe = new BPE(get_data("bpe-models/bpe_code.v0.2"));
  bpe->set_vocabulary(std::vector<std::string>{"Ôø≠10"});
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      bpe,
                      Tokenizer::Flags::JoinerAnnotate);
  test_tok(tokenizer, "A10", "A Ôø≠10");
  test_tok(tokenizer, "A100", "A Ôø≠1Ôø≠ 0Ôø≠ 0");
}

TEST(TokenizerTest, BPEVocabularyWithPreservedTokens) {
  Tokenizer::Options options;
  options.joiner_annotate = true;
  options.joiner = "Ôø≠";

  BPE bpe(get_data("bpe-models/bpe_code.v0.2"));
  bpe.set_vocabulary({"wel"}, &options);

  Token token("welle");
  token.preserve = true;
  token.join_right = true;
  auto subwords = bpe.encode_and_annotate(token);

  ASSERT_EQ(subwords.size(), 5);

  EXPECT_EQ(subwords[0].surface, "w");
  EXPECT_TRUE(subwords[0].join_right);
  EXPECT_FALSE(subwords[0].preserve);

  EXPECT_EQ(subwords[1].surface, "e");
  EXPECT_TRUE(subwords[1].join_right);
  EXPECT_FALSE(subwords[1].preserve);

  EXPECT_EQ(subwords[2].surface, "l");
  EXPECT_TRUE(subwords[2].join_right);
  EXPECT_FALSE(subwords[2].preserve);
}

TEST(TokenizerTest, BPEVocabularyWithSpacer) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Space;
  options.spacer_annotate = true;

  auto bpe = std::make_shared<BPE>(get_data("bpe-models/bpe_code.v0.2"));
  bpe->set_vocabulary(std::vector<std::string>{"‚ñÅwel"}, &options);
  Tokenizer tokenizer(options, bpe);

  test_tok(tokenizer, "die welle", "d i e ‚ñÅwel l e");
}

TEST(TokenizerTest, SpacerAnnotate) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::SpacerAnnotate);
  test_tok_and_detok(tokenizer,
                     "Isn't it so-greatly working?",
                     "Isn ' t ‚ñÅit ‚ñÅso - greatly ‚ñÅworking ?");
  test_tok_and_detok(tokenizer, "MP3", "MP 3");
}

TEST(TokenizerTest, SpacerNew) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      Tokenizer::Flags::SpacerAnnotate | Tokenizer::Flags::SpacerNew);
  test_tok_and_detok(tokenizer,
                     "Isn't it so-greatly working?",
                     "Isn ' t ‚ñÅ it ‚ñÅ so - greatly ‚ñÅ working ?");
}

TEST(TokenizerTest, Alphabets) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::SegmentAlphabetChange);
  std::unordered_map<std::string, size_t> lat_cyrillic_alphabets;
  lat_cyrillic_alphabets["Latin"] = 3;
  lat_cyrillic_alphabets["Cyrillic"] = 1;
  test_tok_alphabet(tokenizer, "raw–ë", "raw –ë", lat_cyrillic_alphabets);

  std::unordered_map<std::string, size_t> han2;
  han2["Han"] = 2;
  test_tok_alphabet(tokenizer, "Êúâ ÂÖ•", "Êúâ ÂÖ•", han2);
}

TEST(TokenizerTest, ArabicAlphabet) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative);
  std::unordered_map<std::string, size_t> alphabets {
    {"Arabic", 5}
  };
  test_tok_alphabet(tokenizer, "ŸÖÿ±ÿ≠ÿ®ÿß", "ŸÖÿ±ÿ≠ÿ®ÿß", alphabets);
}

TEST(TokenizerTest, HalfWidthKanaAlphabet) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative);
  std::unordered_map<std::string, size_t> alphabets = {{"Katakana", 1}};
  test_tok_alphabet(tokenizer, "ÔæÉ", "ÔæÉ", alphabets);
}

TEST(TokenizerTest, NonbreakableSpace) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative);
  test_tok(tokenizer, "a¬†b", "a b");
}

TEST(TokenizerTest, CharMode) {
  Tokenizer tokenizer(Tokenizer::Mode::Char);
  test_tok(tokenizer, "  Hello   World 123.", "H e l l o W o r l d 1 2 3 .");
}

TEST(TokenizerTest, PriorJoinerSupportSpace) {
  Tokenizer tokenizer(Tokenizer::Mode::Space, Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::SupportPriorJoiners | Tokenizer::Flags::PreservePlaceholders);
  test_tok(tokenizer,
           "It is a test-aggressive Ôø≠'Ôø≠ with preÔø≠ tokenizat Ôø≠ions WorldÔø≠ 123 and double Ôø≠ÔΩümrk_placeÔΩ†ÔΩümrk_holderÔΩ†Ôø≠ .",
           "It is a test-aggressive Ôø≠'Ôø≠ with preÔø≠ tokenizat Ôø≠ions WorldÔø≠ 123 and double Ôø≠ ÔΩümrk_placeÔΩ† Ôø≠ ÔΩümrk_holderÔΩ† Ôø≠ .");
}

TEST(TokenizerTest, NormalizeJoinerSpace) {
  Tokenizer tokenizer(Tokenizer::Mode::Space, Tokenizer::Flags::JoinerAnnotate);
  test_tok(tokenizer,
           "It is a test-aggressive ‚ñ†'‚ñ† with preÔø≠ tokenizat ‚ñ†ions ÔΩüentityÔºÉ1ÔºöWorldÔø≠ÔΩ† 123.",
           "It is a test-aggressive ‚ñ†'‚ñ† with pre‚ñ† tokenizat ‚ñ†ions ÔΩüentityÔºÉ1ÔºöWorldÔø≠ÔΩ† 123.");
}

TEST(TokenizerTest, PriorJoinerSupport) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::SupportPriorJoiners);
  test_tok(tokenizer,
           "It is a test-aggressive Ôø≠'Ôø≠ with preÔø≠ tokenizat Ôø≠ions WorldÔø≠ 123.",
           "It is a test Ôø≠-Ôø≠ aggressive Ôø≠'Ôø≠ with preÔø≠ tokenizat Ôø≠ions WorldÔø≠ 123 Ôø≠.");
}

TEST(TokenizerTest, PriorJoinerAndPlaceholder) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::SupportPriorJoiners);
  test_tok(tokenizer, "ÔΩüaÔø≠bÔΩ†", "ÔΩüaÔø≠bÔΩ†");
}

TEST(TokenizerTest, CharModeSpacer) {
  Tokenizer tokenizer(Tokenizer::Mode::Char, Tokenizer::Flags::SpacerAnnotate);
  test_tok(tokenizer, "  Hello   World 123.", "H e l l o ‚ñÅW o r l d ‚ñÅ1 2 3 .");
}

TEST(TokenizerTest, CharModeSpacerNew) {
  Tokenizer tokenizer(Tokenizer::Mode::Char, Tokenizer::Flags::SpacerAnnotate | Tokenizer::Flags::SpacerNew);
  test_tok(tokenizer, "  Hello   World 123.", "H e l l o ‚ñÅ W o r l d ‚ñÅ 1 2 3 .");
}

TEST(TokenizerTest, SentencePiece) {
  Tokenizer tokenizer(Tokenizer::Mode::None, Tokenizer::Flags::SentencePieceModel,
                      get_data("sp-models/sp.model"));
  test_tok_and_detok(tokenizer,
                     "The two shows, called Desire and Secrets, will be one-hour prime-time shows.",
                     "The ‚ñÅtwo ‚ñÅshows , ‚ñÅcalled ‚ñÅDe si re ‚ñÅand ‚ñÅS e c re t s , ‚ñÅwill ‚ñÅbe ‚ñÅone - hour ‚ñÅprime - time ‚ñÅshows .");
}

TEST(TokenizerTest, SentencePieceObject) {
  Tokenizer tokenizer(Tokenizer::Mode::None, new SentencePiece(get_data("sp-models/sp.model")));
  test_tok_and_detok(tokenizer,
                     "The two shows, called Desire and Secrets, will be one-hour prime-time shows.",
                     "The ‚ñÅtwo ‚ñÅshows , ‚ñÅcalled ‚ñÅDe si re ‚ñÅand ‚ñÅS e c re t s , ‚ñÅwill ‚ñÅbe ‚ñÅone - hour ‚ñÅprime - time ‚ñÅshows .");
}

TEST(TokenizerTest, SentencePieceWithJoinersAndPh) {
  Tokenizer tokenizer(Tokenizer::Mode::None,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::SentencePieceModel,
                      get_data("sp-models/sp.model"));
  test_tok_and_detok(tokenizer,
                     "The two shows, called ÔΩüDesireÔΩ† and Secrets, will be one-hour prime-time shows.",
                     "The two shows Ôø≠, called ÔΩüDesireÔΩ† and S Ôø≠e Ôø≠c Ôø≠re Ôø≠t Ôø≠s Ôø≠, will be one Ôø≠- Ôø≠hour prime Ôø≠- Ôø≠time shows Ôø≠.");
  test_tok_and_detok(tokenizer,
                     "The two shows, calledÔΩüDesireÔΩ†and Secrets, will be one-hour prime-time shows.",
                     "The two shows Ôø≠, calledÔø≠ ÔΩüDesireÔΩ†Ôø≠ and S Ôø≠e Ôø≠c Ôø≠re Ôø≠t Ôø≠s Ôø≠, will be one Ôø≠- Ôø≠hour prime Ôø≠- Ôø≠time shows Ôø≠.");
}

TEST(TokenizerTest, SentencePieceWithJoinersAndPh_preserve) {
  Tokenizer tokenizer(Tokenizer::Mode::None,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::SentencePieceModel|
                      Tokenizer::Flags::PreservePlaceholders,
                      get_data("sp-models/sp.model"));
  test_tok_and_detok(tokenizer,
                     "The two shows, calledÔΩüDesireÔΩ†and Secrets, will be one-hour prime-time shows.",
                     "The two shows Ôø≠, calledÔø≠ ÔΩüDesireÔΩ† Ôø≠ and S Ôø≠e Ôø≠c Ôø≠re Ôø≠t Ôø≠s Ôø≠, will be one Ôø≠- Ôø≠hour prime Ôø≠- Ôø≠time shows Ôø≠.");
}

TEST(TokenizerTest, SentencePieceSubwordRegularization) {
  Tokenizer tokenizer(get_data("sp-models/sp_regularization.model"), 1, 0.1);
  test_tok_and_detok(tokenizer,
                     "The two shows, called Desire and Secrets, will be one-hour prime-time shows.",
                     "‚ñÅThe ‚ñÅ two ‚ñÅshow s , ‚ñÅcall ed ‚ñÅDe si re ‚ñÅ and ‚ñÅSec re t s , ‚ñÅw ill ‚ñÅbe ‚ñÅone - h our ‚ñÅ pri me - t im e ‚ñÅshow s .");
}

TEST(TokenizerTest, SentencePieceAlt) {
  Tokenizer tokenizer(Tokenizer::Mode::None, Tokenizer::Flags::SentencePieceModel,
                      get_data("sp-models/wmtende.model"));
  test_tok_and_detok(tokenizer,
                     "Bamford is appealing the sentence and has been granted bail of 50,000 baht.",
                     "‚ñÅBa m ford ‚ñÅis ‚ñÅappealing ‚ñÅthe ‚ñÅsentence ‚ñÅand ‚ñÅhas ‚ñÅbeen ‚ñÅgranted ‚ñÅbail ‚ñÅof ‚ñÅ 50,000 ‚ñÅba ht .");
}

TEST(TokenizerTest, SentencePieceLeadingSpacer) {
  Tokenizer tokenizer(Tokenizer::Mode::None, Tokenizer::Flags::SentencePieceModel,
                      get_data("sp-models/wmtende.model"));
  test_tok_and_detok(tokenizer,
                     "Experts say violence that left 14 adults and seven children dead is nothing more than random chance, not a sign of growing violence in America.",
                     "‚ñÅ Expert s ‚ñÅsay ‚ñÅviolence ‚ñÅthat ‚ñÅleft ‚ñÅ14 ‚ñÅadults ‚ñÅand ‚ñÅseven ‚ñÅchildren ‚ñÅdead ‚ñÅis ‚ñÅnothing ‚ñÅmore ‚ñÅthan ‚ñÅrandom ‚ñÅchance , ‚ñÅnot ‚ñÅa ‚ñÅsign ‚ñÅof ‚ñÅgrowing ‚ñÅviolence ‚ñÅin ‚ñÅAmerica .");
}

TEST(TokenizerTest, SentencePieceWithJoiners) {
  Tokenizer tokenizer(Tokenizer::Mode::None,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::SentencePieceModel,
                      get_data("sp-models/sp.model"));
  test_tok_and_detok(tokenizer,
                     "The two shows, called Desire and Secrets, will be one-hour prime-time shows.",
                     "The two shows Ôø≠, called De Ôø≠si Ôø≠re and S Ôø≠e Ôø≠c Ôø≠re Ôø≠t Ôø≠s Ôø≠, will be one Ôø≠- Ôø≠hour prime Ôø≠- Ôø≠time shows Ôø≠.");
}

TEST(TokenizerTest, AggressiveWithSentencePiece) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::SentencePieceModel,
                      get_data("sp-models/wmtende.model"));
  test_tok(tokenizer,
           "Bamford is appealing the sentence and has been granted bail of 50,000 baht.",
           "Ba m ford is appealing the sentence and has been granted bail of 50 , 000 ba ht .");
}

TEST(TokenizerTest, AggressiveWithSentencePieceAndSpacers) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      Tokenizer::Flags::SpacerAnnotate | Tokenizer::Flags::SentencePieceModel,
                      get_data("sp-models/sp.model"));
  test_tok_and_detok(tokenizer,
                     "The two shows, called Desire and Secrets, will be one-hour prime-time shows.",
                     "The ‚ñÅt wo ‚ñÅs how s , ‚ñÅcalled ‚ñÅD es ir e ‚ñÅand ‚ñÅSe c re t s , ‚ñÅwill ‚ñÅbe ‚ñÅone - hour ‚ñÅp rime - time ‚ñÅs how s .");
}

TEST(TokenizerTest, AggressiveWithSentencePieceAndJoiners) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::SentencePieceModel,
                      get_data("sp-models/sp.model"));
  test_tok_and_detok(tokenizer,
                     "The two shows, called Desire and Secrets, will be one-hour prime-time shows.",
                     "The t Ôø≠wo s Ôø≠how Ôø≠s Ôø≠, called D Ôø≠es Ôø≠ir Ôø≠e and Se Ôø≠c Ôø≠re Ôø≠t Ôø≠s Ôø≠, will be one Ôø≠-Ôø≠ hour p Ôø≠rime Ôø≠-Ôø≠ time s Ôø≠how Ôø≠s Ôø≠.");
}

TEST(TokenizerTest, SentencePieceIsolatedSpacer)
{
  Tokenizer tokenizer(Tokenizer::Mode::None,
                      Tokenizer::Flags::SentencePieceModel | Tokenizer::Flags::PreservePlaceholders,
                      get_data("sp-models/wmtende.model"));
  test_tok(tokenizer, "a crumpled sofa", "‚ñÅa ‚ñÅ cru mpl ed ‚ñÅsofa");
  test_tok(tokenizer, "a ÔΩüphÔΩ†crumpled sofa", "‚ñÅa ‚ñÅ ÔΩüphÔΩ† cru mpl ed ‚ñÅsofa");
}

TEST(TokenizerTest, SentencePieceIsolatedSpacerAndJoinerAnnotate)
{
  Tokenizer tokenizer(Tokenizer::Mode::None,
                      Tokenizer::Flags::SentencePieceModel
                      | Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::PreservePlaceholders,
                      get_data("sp-models/wmtende.model"));
  test_tok(tokenizer, "a crumpled sofa", "a cru Ôø≠mpl Ôø≠ed sofa");
  test_tok(tokenizer, "a ÔΩüphÔΩ†crumpled sofa", "a ÔΩüphÔΩ† Ôø≠ cru Ôø≠mpl Ôø≠ed sofa");
  test_tok(tokenizer, "ÔΩüphÔΩ†,", "ÔΩüphÔΩ† Ôø≠ ,");
}

TEST(TokenizerTest, AggressiveWithSentencePieceIsolatedSpacerAndJoinerAnnotate) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      Tokenizer::Flags::SentencePieceModel | Tokenizer::Flags::JoinerAnnotate,
                      get_data("sp-models/wmtende.model"));
  test_tok(tokenizer, "depending on its temperature.", "depending on its temperature Ôø≠.");
}

TEST(TokenizerTest, WithoutVocabulary) {
  Tokenizer tokenizer(Tokenizer::Mode::Space,
                      Tokenizer::Flags::JoinerAnnotate,
                      get_data("bpe-models/bpe_code.v0.2"),
                      "@@");
  test_tok(tokenizer,
           "Oliver Gr√ºn , welle",
           "Oliver Gr√ºn , welle");
}

TEST(TokenizerTest, WithVocabulary) {
  Tokenizer tokenizer(Tokenizer::Mode::Space,
                      Tokenizer::Flags::JoinerAnnotate,
                      get_data("bpe-models/bpe_code.v0.2"),
                      "@@",
                      get_data("bpe-models/vocab.en"),
                      50);
  test_tok(tokenizer,
           "Oliver Gr√ºn , welle",
           "Oliver Gr@@ √º@@ n , wel@@ le");
}

TEST(TokenizerTest, WithVocabularyTabSeparated) {
  Tokenizer tokenizer(Tokenizer::Mode::Space,
                      Tokenizer::Flags::JoinerAnnotate,
                      get_data("bpe-models/bpe_code.v0.2"),
                      "@@",
                      get_data("bpe-models/vocab.en.tab"),
                      50);
  test_tok(tokenizer,
           "Oliver Gr√ºn , welle",
           "Oliver Gr@@ √º@@ n , wel@@ le");
}

TEST(TokenizerTest, TokenInterface)
{
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::CaseMarkup);
  const std::string text = "Hello world!";
  std::vector<Token> tokens;
  tokenizer.tokenize(text, tokens);
  EXPECT_EQ(tokens[0].surface, "hello");
  EXPECT_EQ(tokens[1].surface, "world");
  EXPECT_EQ(tokens[2].surface, "!");
  EXPECT_EQ(tokenizer.detokenize(tokens), text);
}

TEST(TokenizerTest, DetokenizeEmptyToken)
{
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::JoinerAnnotate);
  const std::vector<std::string> tokens = { "a", "", "b" };
  EXPECT_EQ(tokenizer.detokenize(tokens), "a b");
}

int main(int argc, char *argv[]) {
  testing::InitGoogleTest(&argc, argv);
  assert(argc == 2);
  data_dir = argv[1];
  return RUN_ALL_TESTS();
}
