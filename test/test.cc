#include <gtest/gtest.h>

#include <onmt/BPE.h>
#include <onmt/SentencePiece.h>
#include <onmt/Tokenizer.h>

#include <unicode/unistr.h>
#include <unicode/normalizer2.h>

using namespace onmt;

static std::string normalize_nfc(const std::string& s) {
  UErrorCode error = U_ZERO_ERROR;
  const auto* norm = icu::Normalizer2::getNFCInstance(error);
  EXPECT_TRUE(U_SUCCESS(error));

  icu::UnicodeString u = icu::UnicodeString::fromUTF8(s);
  icu::UnicodeString out;
  norm->normalize(u, out, error);
  EXPECT_TRUE(U_SUCCESS(error));

  std::string result;
  out.toUTF8String(result);
  return result;
}

static std::string data_dir;

static std::string get_data(const std::string& path) {
  return data_dir + "/" + path;
}

static void test_tok(const Tokenizer& tokenizer,
                     const std::string& in,
                     const std::string& expected,
                     bool detokenize = false,
                     bool training = true) {
  std::vector<std::string> tokens;
  std::vector<std::vector<std::string>> features;
  tokenizer.tokenize(in, tokens, features, training);
  EXPECT_EQ(write_tokens(tokens, features), expected);
  if (detokenize) {
    EXPECT_EQ(tokenizer.detokenize(tokens, features), in);
  }
}

static void test_tok(const Tokenizer::Options& options,
                     const std::string& in,
                     const std::string& expected,
                     bool detokenize = false,
                     bool training = true) {
  test_tok(Tokenizer(options), in, expected, detokenize, training);
}

static void test_tok(const Tokenizer& tokenizer,
                     const std::string& in,
                     const std::vector<std::string>& expected,
                     bool detokenize = false) {
  std::vector<std::string> tokens;
  tokenizer.tokenize(in, tokens);
  ASSERT_EQ(tokens.size(), expected.size());
  for (size_t i = 0; i < tokens.size(); ++i) {
    EXPECT_EQ(tokens[i], expected[i])  << "Unexpected token mismatch at index " << i;
  }
  if (detokenize) {
    auto text = tokenizer.detokenize(tokens);
    EXPECT_EQ(text, in);
  }
}

static void test_tok(const Tokenizer::Options& options,
                     const std::string& in,
                     const std::vector<std::string>& expected,
                     bool detokenize = false) {
  return test_tok(Tokenizer(options), in, expected, detokenize);
}

static void test_detok(const Tokenizer& tokenizer,
                       const std::string& in,
                       const std::string& expected) {
  std::vector<std::string> tokens;
  std::vector<std::vector<std::string>> features;
  read_tokens(in, tokens, features);
  EXPECT_EQ(
    normalize_nfc(tokenizer.detokenize(tokens, features)),
    normalize_nfc(expected)
  );
}

static void test_detok(const Tokenizer::Options& options,
                       const std::string& in,
                       const std::string& expected) {
  return test_detok(Tokenizer(options), in, expected);
}

static void test_tok_alphabet(const Tokenizer::Options& options,
                              const std::string& in,
                              const std::string& expected,
                              const std::unordered_map<std::string, size_t>& expected_alphabets) {
  std::vector<std::string> words;
  std::vector<std::vector<std::string> > features;
  std::unordered_map<std::string, size_t> alphabets;

  Tokenizer(options).tokenize(in, words, features, alphabets);

  std::string output;
  for (size_t i = 0; i < words.size(); ++i)
  {
    if (i > 0)
      output += " ";
    output += words[i];
  }

  EXPECT_EQ(output, expected);

  for (const auto& it: expected_alphabets) {
    ASSERT_TRUE(alphabets.find(it.first) != alphabets.end());
    EXPECT_EQ(alphabets[it.first], it.second);
  }
}

static void test_tok_and_detok(const Tokenizer& tokenizer,
                               const std::string& in,
                               const std::string& expected) {
  return test_tok(tokenizer, in, expected, true);
}

static void test_tok_and_detok(const Tokenizer::Options& options,
                               const std::string& in,
                               const std::string& expected) {
  return test_tok_and_detok(Tokenizer(options), in, expected);
}


TEST(TokenizerTest, DetokenizeEmptyToken) {
  Tokenizer tokenizer({});
  EXPECT_EQ(tokenizer.detokenize({"a", "", "b"}), "a b");
}

TEST(TokenizerTest, DetokenizeWithRanges) {
  Tokenizer tokenizer({});
  Ranges ranges;
  tokenizer.detokenize({"Helï¿­", "lo", "ï½Ÿmrk_case_modifier_Cï½ ", "w", "ï¿­", "orld", "ï¿­!"}, ranges);
  // Result: Hello World!
  ASSERT_EQ(ranges.size(), 5);
  EXPECT_EQ(ranges[0], (std::pair<size_t, size_t>(0, 2)));
  EXPECT_EQ(ranges[1], (std::pair<size_t, size_t>(3, 4)));
  EXPECT_EQ(ranges[3], (std::pair<size_t, size_t>(6, 6)));
  EXPECT_EQ(ranges[5], (std::pair<size_t, size_t>(7, 10)));
  EXPECT_EQ(ranges[6], (std::pair<size_t, size_t>(11, 11)));
}

TEST(TokenizerTest, DetokenizeWithMergedRanges) {
  Tokenizer tokenizer({});
  Ranges ranges;
  tokenizer.detokenize({"Helï¿­", "lo", "wï¿­", "orld", "ï¿­!"}, ranges, true);
  // Result: Hello World!
  ASSERT_EQ(ranges.size(), 5);
  EXPECT_EQ(ranges[0], (std::pair<size_t, size_t>(0, 4)));
  EXPECT_EQ(ranges[1], (std::pair<size_t, size_t>(0, 4)));
  EXPECT_EQ(ranges[2], (std::pair<size_t, size_t>(6, 10)));
  EXPECT_EQ(ranges[3], (std::pair<size_t, size_t>(6, 10)));
  EXPECT_EQ(ranges[4], (std::pair<size_t, size_t>(11, 11)));
}

TEST(TokenizerTest, DetokenizeWithMergedRangesPlaceholders) {
  Tokenizer tokenizer({});
  Ranges ranges;
  tokenizer.detokenize({"ï½Ÿaï½ ï¿­", "b", "ï¿­ï½Ÿcï½ "}, ranges, true);
  // Result: ï½Ÿaï½ bï½Ÿcï½ 
  ASSERT_EQ(ranges.size(), 3);
  EXPECT_EQ(ranges[0], (std::pair<size_t, size_t>(0, 6)));
  EXPECT_EQ(ranges[1], (std::pair<size_t, size_t>(7, 7)));
  EXPECT_EQ(ranges[2], (std::pair<size_t, size_t>(8, 14)));
}

TEST(TokenizerTest, Empty) {
  test_tok({}, "", "");
}

TEST(TokenizerTest, NonbreakableSpace) {
  test_tok({}, "aÂ b", "a b");
}

TEST(TokenizerTest, Conservative) {
  test_tok({},
           "Your Hardware-Enablement Stack (HWE) is supported until April 2019.",
           "Your Hardware-Enablement Stack ( HWE ) is supported until April 2019 .");
}

TEST(TokenizerTest, None) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::None;
  test_tok(options, "Hello World!", "Hello World!");
}

TEST(TokenizerTest, NoneWithPlaceholders) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::None;
  options.joiner_annotate = true;
  test_tok(options, "Hello:ï½ŸWorldï½ !", "Hello:ï¿­ ï½ŸWorldï½ ï¿­ !");
}

TEST(TokenizerTest, NonePlaceholderSpacesEscape) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::None;
  test_tok(options, "ï½Ÿa b cï½ ", "ï½Ÿaï¼…0020bï¼…0020cï½ ");
}

TEST(TokenizerTest, NonePlaceholderSpacesNoEscape) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::None;
  options.no_substitution = true;
  test_tok(options, "ï½Ÿa b cï½ ", "ï½Ÿa b cï½ ");
}

TEST(TokenizerTest, NonePreservePlaceholders) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::None;
  options.joiner_annotate = true;
  options.preserve_placeholders = true;
  test_tok(options, "Hello:ï½ŸWorldï½ !", "Hello:ï¿­ ï½ŸWorldï½  ï¿­ !");
}

TEST(TokenizerTest, NonePreserveTokens) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::None;
  options.joiner_annotate = true;
  options.preserve_segmented_tokens = true;
  test_tok(options, "Helloï½ŸWorldï½ !", "Hello ï¿­ ï½ŸWorldï½  ï¿­ !");
}

TEST(TokenizerTest, Space) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Space;
  test_tok(options,
           "49th meeting Social and human rights questions: human rights [14 (g)]",
           "49th meeting Social and human rights questions: human rights [14 (g)]");
}

TEST(TokenizerTest, SpaceEmpty) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Space;
  test_tok(options, "", "");
}

TEST(TokenizerTest, SpaceSingle) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Space;
  test_tok(options, "Hello", "Hello");
}

TEST(TokenizerTest, SpaceLeading) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Space;
  test_tok(options, " Hello", "Hello");
}

TEST(TokenizerTest, SpaceTrailing) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Space;
  test_tok(options, "Hello ", "Hello");
}

TEST(TokenizerTest, SpaceDuplicated) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Space;
  test_tok(options, "  Hello   World ", "Hello World");
}

TEST(TokenizerTest, SpacePlaceholderSpacesEscape) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Space;
  options.joiner_annotate = true;
  test_tok(options, "aï½Ÿb cï½  d", "aï¿­ ï½Ÿbï¼…0020cï½  d");
}

TEST(TokenizerTest, SpaceWithFeatures) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Space;
  options.case_feature = true;
  Tokenizer tokenizer(options);
  std::vector<std::string> tokens;
  std::vector<std::vector<std::string>> features;
  tokenizer.tokenize("Helloï¿¨12ï¿¨AB worldï¿¨34ï¿¨CD", tokens, features);
  EXPECT_EQ(tokens, (std::vector<std::string>{"hello", "world"}));
  ASSERT_EQ(features.size(), 3);
  EXPECT_EQ(features[0], (std::vector<std::string>{"12", "34"}));
  EXPECT_EQ(features[1], (std::vector<std::string>{"AB", "CD"}));
  EXPECT_EQ(features[2], (std::vector<std::string>{"C", "L"}));
}

TEST(TokenizerTest, Char) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Char;
  test_tok(options, "  Hello   World 123.", "H e l l o W o r l d 1 2 3 .");
}

TEST(TokenizerTest, CharWithSpacer) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Char;
  options.spacer_annotate = true;
  test_tok(options, "  Hello   World 123.", "H e l l o â–W o r l d â–1 2 3 .");
}

TEST(TokenizerTest, CharWithSpacerNew) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Char;
  options.spacer_annotate = true;
  options.spacer_new = true;
  test_tok(options, "  Hello   World 123.", "H e l l o â– W o r l d â– 1 2 3 .");
}

TEST(TokenizerTest, JoinerAnnotate) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  options.joiner_annotate = true;
  test_tok_and_detok(options,
                     "Isn't it so-greatly working?",
                     "Isn ï¿­'ï¿­ t it so ï¿­-ï¿­ greatly working ï¿­?");
  test_tok_and_detok(options, "MP3", "MP ï¿­3");
  test_tok_and_detok(options, "A380", "A ï¿­380");
  test_tok_and_detok(options, "$1", "$ï¿­ 1");
}

TEST(TokenizerTest, PriorJoinerSupportSpace) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Space;
  options.joiner_annotate = true;
  options.support_prior_joiners = true;
  options.preserve_placeholders = true;
  test_tok(options,
           "It is a test-aggressive ï¿­'ï¿­ with preï¿­ tokenizat ï¿­ions Worldï¿­ 123 and double ï¿­ï½Ÿmrk_placeï½ ï½Ÿmrk_holderï½ ï¿­ .",
           "It is a test-aggressive ï¿­'ï¿­ with preï¿­ tokenizat ï¿­ions Worldï¿­ 123 and double ï¿­ ï½Ÿmrk_placeï½  ï¿­ ï½Ÿmrk_holderï½  ï¿­ .");
}

TEST(TokenizerTest, PriorJoinerSupport) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  options.joiner_annotate = true;
  options.support_prior_joiners = true;
  test_tok(options,
           "It is a test-aggressive ï¿­'ï¿­ with preï¿­ tokenizat ï¿­ions Worldï¿­ 123.",
           "It is a test ï¿­-ï¿­ aggressive ï¿­'ï¿­ with preï¿­ tokenizat ï¿­ions Worldï¿­ 123 ï¿­.");
}

TEST(TokenizerTest, PriorJoinerAndPlaceholder) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  options.joiner_annotate = true;
  options.support_prior_joiners = true;
  test_tok(options, "ï½Ÿaï¿­bï½ ", "ï½Ÿaï¿­bï½ ");
}

TEST(TokenizerTest, SpacerAnnotate) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  options.spacer_annotate = true;
  test_tok_and_detok(options,
                     "Isn't it so-greatly working?",
                     "Isn ' t â–it â–so - greatly â–working ?");
  test_tok_and_detok(options, "MP3", "MP 3");
}

TEST(TokenizerTest, SpacerNew) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  options.spacer_annotate = true;
  options.spacer_new = true;
  test_tok_and_detok(options,
                     "Isn't it so-greatly working?",
                     "Isn ' t â– it â– so - greatly â– working ?");
}

TEST(TokenizerTest, ProtectedSequence) {
  Tokenizer::Options options;
  options.joiner_annotate = true;
  test_tok_and_detok(options, "ï½Ÿ1,023ï½ km", "ï½Ÿ1,023ï½ ï¿­ km");
  test_tok_and_detok(options, "Aï½Ÿ380ï½ ", "A ï¿­ï½Ÿ380ï½ ");
  test_tok_and_detok(options, "ï½Ÿ1,023ï½ ï½Ÿ380ï½ ", "ï½Ÿ1,023ï½ ï¿­ ï½Ÿ380ï½ ");
  test_tok_and_detok(options, "ï½Ÿ1023ï½ .", "ï½Ÿ1023ï½  ï¿­.");
  test_tok_and_detok(options, "$ï½Ÿ0.23ï½ ", "$ï¿­ ï½Ÿ0.23ï½ ");
  test_tok_and_detok(options, "ï½Ÿ0.23ï½ $", "ï½Ÿ0.23ï½  ï¿­$");
  test_tok_and_detok(options, "ï½ŸUS$ï½ 23", "ï½ŸUS$ï½ ï¿­ 23");
  test_tok_and_detok(options, "1ï½ŸABCDï½ 0", "1 ï¿­ï½ŸABCDï½ ï¿­ 0");
}

TEST(TokenizerTest, PreserveProtectedSequence) {
  Tokenizer::Options options;
  options.joiner_annotate = true;
  options.preserve_placeholders = true;
  test_tok_and_detok(options, "ï½Ÿ1,023ï½ km", "ï½Ÿ1,023ï½  ï¿­ km");
  test_tok_and_detok(options, "Aï½Ÿ380ï½ ", "A ï¿­ ï½Ÿ380ï½ ");
  test_tok_and_detok(options, "ï½Ÿ1,023ï½ ï½Ÿ380ï½ ", "ï½Ÿ1,023ï½  ï¿­ ï½Ÿ380ï½ ");
  test_tok_and_detok(options, "ï½Ÿ1023ï½ .", "ï½Ÿ1023ï½  ï¿­.");
  test_tok_and_detok(options, "$ï½Ÿ0.23ï½ ", "$ï¿­ ï½Ÿ0.23ï½ ");
  test_tok_and_detok(options, "ï½Ÿ0.23ï½ $", "ï½Ÿ0.23ï½  ï¿­$");
  test_tok_and_detok(options, "ï½ŸUS$ï½ 23", "ï½ŸUS$ï½  ï¿­ 23");
  test_tok_and_detok(options, "1ï½ŸABCDï½ 0", "1 ï¿­ ï½ŸABCDï½  ï¿­ 0");
}

TEST(TokenizerTest, PreserveProtectedSequenceSpacerAnnotate) {
  Tokenizer::Options options;
  options.spacer_annotate = true;
  options.preserve_placeholders = true;
  test_tok_and_detok(options, "ï½Ÿ1,023ï½ km", "ï½Ÿ1,023ï½  km");
  test_tok_and_detok(options, "A ï½Ÿ380ï½ ", "A â– ï½Ÿ380ï½ ");
  test_tok_and_detok(options, "ï½Ÿ1,023ï½ ï½Ÿ380ï½ ", "ï½Ÿ1,023ï½  ï½Ÿ380ï½ ");
  test_tok_and_detok(options, "ï½Ÿ1,023ï½  ï½Ÿ380ï½ ", "ï½Ÿ1,023ï½  â– ï½Ÿ380ï½ ");
  test_tok_and_detok(options, "ï½Ÿ1023ï½ .", "ï½Ÿ1023ï½  .");
  test_tok_and_detok(options, "ï½Ÿ1023ï½  .", "ï½Ÿ1023ï½  â–.");
}

TEST(TokenizerTest, ProtectedSequenceAggressive) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  options.joiner_annotate = true;
  test_tok_and_detok(options, "ï½Ÿ1,023ï½ km", "ï½Ÿ1,023ï½ ï¿­ km");
  test_tok_and_detok(options, "Aï½Ÿ380ï½ ", "A ï¿­ï½Ÿ380ï½ ");
  test_tok_and_detok(options, "ï½Ÿ1,023ï½ ï½Ÿ380ï½ ", "ï½Ÿ1,023ï½ ï¿­ ï½Ÿ380ï½ ");
  test_tok_and_detok(options, "ï½Ÿ1023ï½ .", "ï½Ÿ1023ï½  ï¿­.");
  test_tok_and_detok(options, "$ï½Ÿ0.23ï½ ", "$ï¿­ ï½Ÿ0.23ï½ ");
  test_tok_and_detok(options, "ï½Ÿ0.23ï½ $", "ï½Ÿ0.23ï½  ï¿­$");
  test_tok_and_detok(options, "ï½ŸUS$ï½ 23", "ï½ŸUS$ï½ ï¿­ 23");
  test_tok_and_detok(options, "1ï½ŸABCDï½ 0", "1 ï¿­ï½ŸABCDï½ ï¿­ 0");
}

TEST(TokenizerTest, ProtectedSequenceJoinerNew) {
  Tokenizer::Options options;
  options.joiner_annotate = true;
  options.joiner_new = true;
  test_tok_and_detok(options, "ï½Ÿ1,023ï½ km", "ï½Ÿ1,023ï½  ï¿­ km");
  test_tok_and_detok(options, "Aï½Ÿ380ï½ ", "A ï¿­ ï½Ÿ380ï½ ");
  test_tok_and_detok(options, "ï½Ÿ1,023ï½ ï½Ÿ380ï½ ", "ï½Ÿ1,023ï½  ï¿­ ï½Ÿ380ï½ ");
  test_tok_and_detok(options, "ï½Ÿ1023ï½ .", "ï½Ÿ1023ï½  ï¿­ .");
}

TEST(TokenizerTest, Substitution) {
  test_tok({},
           "testï¿­ protectï¿¨, ï¼š, â–, and ï¼… or ï¼ƒ...",
           "test â–  protect â”‚ , : , _ , and % or # . . .");
  test_tok({}, "ï½Ÿtagï¼švalue with spacesï½ ", "ï½Ÿtagï¼švalueï¼…0020withï¼…0020spacesï½ ");
}

TEST(TokenizerTest, NoSubstitution) {
  Tokenizer::Options options;
  options.no_substitution = true;
  test_tok(options,
           "testï¿­ protectï¿¨, ï¼š, â–, and ï¼… or ï¼ƒ...",
           "test ï¿­ protect ï¿¨ , ï¼š , â– , and ï¼… or ï¼ƒ . . .");
  test_tok(options, "ï½Ÿtagï¼švalue with spacesï½ ", "ï½Ÿtagï¼švalue with spacesï½ ");
}

TEST(TokenizerTest, WithSeparators) {
  Tokenizer::Options options;
  options.with_separators = true;
  test_tok(options, "HelloÂ World!", {"Hello", "Â ", "World", "!"}, true);
  test_detok(options, "Hello   World !", "Hello World!");
  test_detok(options, "Hello     World !", "Hello  World!");
}

TEST(TokenizerTest, JoinerSubstitution) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Space;
  options.joiner_annotate = true;
  test_tok(options,
           "It is a test-aggressive â– 'â–  with preï¿­ tokenizat â– ions ï½Ÿentityï¼ƒ1ï¼šWorldï¿­ï½  123.",
           "It is a test-aggressive â– 'â–  with preâ–  tokenizat â– ions ï½Ÿentityï¼ƒ1ï¼šWorldï¿­ï½  123.");
}

TEST(TokenizerTest, InvalidEscapeSequence) {
  test_detok({},
             "è¦æ±‚ä»ï¼‘ï¼”ï¼…é™åˆ°ï¼’ï¼…ï¼ŒåŠ¨å‘˜å¹²éƒ¨å‚åŠ ç”Ÿäº§ï¼Œå‘åˆä½œç¤¾çœ‹é½ã€‚",
             "è¦æ±‚ä»ï¼‘ï¼”ï¼…é™åˆ°ï¼’ï¼…ï¼ŒåŠ¨å‘˜å¹²éƒ¨å‚åŠ ç”Ÿäº§ï¼Œå‘åˆä½œç¤¾çœ‹é½ã€‚");
  test_detok({},
             "ç‹é´»è–‡èªªï¼Œå¾©æ˜Ÿåƒ…æŒæœ‰0.7ï¼…BNTè‚¡æ¬Šï¼Œæ‰€ä»¥å¾·åœ‹BNTæ˜¯å¾·åœ‹å…¬å¸ç„¡èª¤",
             "ç‹é´»è–‡èªªï¼Œå¾©æ˜Ÿåƒ…æŒæœ‰0.7ï¼…BNTè‚¡æ¬Šï¼Œæ‰€ä»¥å¾·åœ‹BNTæ˜¯å¾·åœ‹å…¬å¸ç„¡èª¤");
}

TEST(TokenizerTest, ZeroWidthJoiner) {
  Tokenizer::Options options;
  options.joiner_annotate = true;
  test_tok_and_detok(options, "ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦", "ğŸ‘¨ ï¿­â€ ï¿­ğŸ‘© ï¿­â€ ï¿­ğŸ‘¦");
}

TEST(TokenizerTest, CombiningMark) {
  Tokenizer::Options options;
  options.joiner_annotate = true;
  test_tok_and_detok(options,
                     "à¤µà¤°à¥à¤¤à¤®à¤¾à¤¨ à¤²à¤¿à¤ªà¤¿ (à¤¸à¥à¤•à¥à¤°à¤¿à¤ªà¥à¤Ÿ) à¤–à¥‹ à¤œà¤¾à¤à¤—à¥€à¥¤",
                     "à¤µà¤°à¥à¤¤à¤®à¤¾à¤¨ à¤²à¤¿à¤ªà¤¿ (ï¿­ à¤¸à¥à¤•à¥à¤°à¤¿à¤ªà¥à¤Ÿ ï¿­) à¤–à¥‹ à¤œà¤¾à¤à¤—à¥€ ï¿­à¥¤");
}

TEST(TokenizerTest, CombiningMarkOnSpace) {
  {
    Tokenizer::Options options;
    options.joiner_annotate = true;
    test_tok_and_detok(options, "b Ì‡c", "b ï¿­ï¼…0020Ì‡ï¿­ c");
  }

  {
    Tokenizer::Options options;
    options.joiner_annotate = true;
    options.allow_isolated_marks = true;
    test_tok_and_detok(options, "b Ì‡c", "b Ì‡ï¿­ c");
  }

  {
    Tokenizer::Options options;
    options.spacer_annotate = true;
    test_tok_and_detok(options, "b Ì‡c", "b ï¼…0020Ì‡ c");
  }
}

TEST(TokenizerTest, CombiningMarkOnSpaceNoSubstitution) {
  Tokenizer::Options options;
  options.joiner_annotate = true;
  options.no_substitution = true;
  test_tok(options, "angles á§1 and á§2", {"angles", "ï¿­ á§ï¿­", "1", "and", "ï¿­ á§ï¿­", "2"}, true);
}

TEST(TokenizerTest, CombiningMarkAfterPlaceholder) {
  Tokenizer::Options options;
  options.joiner_annotate = true;
  options.preserve_placeholders = true;
  test_tok_and_detok(options, "ï½Ÿaï½ ×‚b", "ï½Ÿaï½  ï¿­×‚ï¿­ b");
}

TEST(TokenizerTest, Alphabets) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  options.segment_alphabet_change = true;
  test_tok_alphabet(options, "rawĞ‘", "raw Ğ‘", {{"Latin", 3}, {"Cyrillic", 1}});
  test_tok_alphabet(options, "æœ‰ å…¥", "æœ‰ å…¥", {{"Han", 2}});
}

TEST(TokenizerTest, ArabicAlphabet) {
  test_tok_alphabet({}, "Ù…Ø±Ø­Ø¨Ø§", "Ù…Ø±Ø­Ø¨Ø§", {{"Arabic", 5}});
}

TEST(TokenizerTest, HalfWidthKanaAlphabet) {
  test_tok_alphabet({}, "ï¾ƒ", "ï¾ƒ", {{"Katakana", 1}});
}

TEST(TokenizerTest, CaseFeature) {
  Tokenizer::Options options;
  options.case_feature = true;
  options.joiner_annotate = true;
  // Note: in C literal strings, \ is escaped by another \.
  test_tok(options,
           "test \\\\\\\\a Capitalized lowercased UPPERCASÃ‰ miXÃªd - cyrillic-Ğ‘",
           "testï¿¨L \\ï¿¨N ï¿­\\ï¿¨N ï¿­\\ï¿¨N ï¿­\\ï¿­ï¿¨N aï¿¨L capitalizedï¿¨C lowercasedï¿¨L uppercasÃ©ï¿¨U mixÃªdï¿¨M -ï¿¨N cyrillic-Ğ±ï¿¨M");
}

TEST(TokenizerTest, CaseFeatureWithJoinerNew) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  options.case_feature = true;
  options.joiner_annotate = true;
  options.joiner_new = true;
  test_tok(options, "a-b.", "aï¿¨L ï¿­ï¿¨N -ï¿¨N ï¿­ï¿¨N bï¿¨L ï¿­ï¿¨N .ï¿¨N");
}

TEST(TokenizerTest, CaseFeatureWithSpacerNew) {
  Tokenizer::Options options;
  options.case_feature = true;
  options.spacer_annotate = true;
  options.spacer_new = true;
  test_tok(options, "a b", "aï¿¨L â–ï¿¨N bï¿¨L");
}

TEST(TokenizerTest, CaseMarkupWithJoiners) {
  Tokenizer::Options options;
  options.case_markup = true;
  options.joiner_annotate = true;
  test_tok_and_detok(options,
                     "Hello world!", "ï½Ÿmrk_case_modifier_Cï½  hello world ï¿­!");
  test_tok_and_detok(options,
                     "Hello WORLD!", "ï½Ÿmrk_case_modifier_Cï½  hello ï½Ÿmrk_begin_case_region_Uï½  world ï½Ÿmrk_end_case_region_Uï½  ï¿­!");
  test_tok_and_detok(options,
                     "HELLO WORLD!", "ï½Ÿmrk_begin_case_region_Uï½  hello ï½Ÿmrk_end_case_region_Uï½  ï½Ÿmrk_begin_case_region_Uï½  world ï½Ÿmrk_end_case_region_Uï½  ï¿­!");
  test_tok_and_detok(options,
                     "Hello WOrld!", "ï½Ÿmrk_case_modifier_Cï½  hello ï½Ÿmrk_begin_case_region_Uï½  woï¿­ ï½Ÿmrk_end_case_region_Uï½  rld ï¿­!");
  test_tok_and_detok(options,
                     "hello woRld!", "hello woï¿­ ï½Ÿmrk_case_modifier_Cï½  rld ï¿­!");
  test_tok_and_detok(options,
                     "hello woRlD!", "hello woï¿­ ï½Ÿmrk_case_modifier_Cï½  rlï¿­ ï½Ÿmrk_case_modifier_Cï½  d ï¿­!");
}

TEST(TokenizerTest, CaseMarkupWithSoftUppercaseRegions) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  options.case_markup = true;
  options.soft_case_regions = true;
  options.joiner_annotate = true;
  test_tok_and_detok(options,
                     "AA.BB", "ï½Ÿmrk_begin_case_region_Uï½  aa ï¿­.ï¿­ bb ï½Ÿmrk_end_case_region_Uï½ ");
  test_tok_and_detok(options,
                     "A BC", "ï½Ÿmrk_begin_case_region_Uï½  a bc ï½Ÿmrk_end_case_region_Uï½ ");
  test_tok_and_detok(options,
                     "AA.", "ï½Ÿmrk_begin_case_region_Uï½  aa ï½Ÿmrk_end_case_region_Uï½  ï¿­.");
  test_tok_and_detok(options,
                     "A-B/C", "ï½Ÿmrk_begin_case_region_Uï½  a ï¿­-ï¿­ b ï¿­/ï¿­ c ï½Ÿmrk_end_case_region_Uï½ ");
  test_tok_and_detok(options,
                     "A-B/c", "ï½Ÿmrk_begin_case_region_Uï½  a ï¿­-ï¿­ b ï½Ÿmrk_end_case_region_Uï½  ï¿­/ï¿­ c");
  test_tok_and_detok(options,
                     "A", "ï½Ÿmrk_case_modifier_Cï½  a");
  test_tok_and_detok(options,
                     "A-", "ï½Ÿmrk_case_modifier_Cï½  a ï¿­-");
  test_tok_and_detok(options,
                     "ID: A23X52,",
                     "ï½Ÿmrk_begin_case_region_Uï½  id ï¿­: a ï¿­23ï¿­ x ï¿­52 ï½Ÿmrk_end_case_region_Uï½  ï¿­,");
  test_tok_and_detok(options,
                     "Show PP-LX-DP",
                     "ï½Ÿmrk_case_modifier_Cï½  show ï½Ÿmrk_begin_case_region_Uï½  pp ï¿­-ï¿­ lx ï¿­-ï¿­ dp ï½Ÿmrk_end_case_region_Uï½ ");
  test_tok_and_detok(options,
                     "AA ï½ŸBBï½  CC", "ï½Ÿmrk_begin_case_region_Uï½  aa ï½Ÿmrk_end_case_region_Uï½  ï½ŸBBï½  ï½Ÿmrk_begin_case_region_Uï½  cc ï½Ÿmrk_end_case_region_Uï½ ");
}

TEST(TokenizerTest, CaseMarkupWithJoinerNew) {
  Tokenizer::Options options;
  options.case_markup = true;
  options.joiner_annotate = true;
  options.joiner_new = true;
  test_detok(options, "hello ï½Ÿmrk_case_modifier_Cï½  ï¿­ world !", "helloWorld !");
  test_detok(options, "hello ï½Ÿmrk_case_modifier_Cï½  ï¿­", "hello");
}

TEST(TokenizerTest, CaseMarkupWithSpacers) {
  Tokenizer::Options options;
  options.case_markup = true;
  options.spacer_annotate = true;
  test_tok_and_detok(options,
                     "Hello world!", "ï½Ÿmrk_case_modifier_Cï½  hello â–world !");
  test_tok_and_detok(options,
                     "Hello WORLD!", "ï½Ÿmrk_case_modifier_Cï½  hello ï½Ÿmrk_begin_case_region_Uï½  â–world ï½Ÿmrk_end_case_region_Uï½  !");
  test_tok_and_detok(options,
                     "Hello WOrld!", "ï½Ÿmrk_case_modifier_Cï½  hello ï½Ÿmrk_begin_case_region_Uï½  â–wo ï½Ÿmrk_end_case_region_Uï½  rld !");
  test_tok_and_detok(options,
                     "hello woRld!", "hello â–wo ï½Ÿmrk_case_modifier_Cï½  rld !");
}

TEST(TokenizerTest, CaseMarkupWithSpacerNew) {
  Tokenizer::Options options;
  options.case_markup = true;
  options.spacer_annotate = true;
  options.spacer_new = true;
  test_detok(options, "hello ï½Ÿmrk_case_modifier_Cï½  â– world !", "hello World!");
  test_detok(options, "hello ï½Ÿmrk_case_modifier_Cï½  â–", "hello ");
}

TEST(TokenizerTest, CaseMarkupWithBPE) {
  Tokenizer::Options options;
  options.case_markup = true;
  options.joiner_annotate = true;
  Tokenizer tokenizer(options,
                      std::make_shared<BPE>(get_data("bpe-models/codes_suffix_case_insensitive.fr")));
  test_tok_and_detok(tokenizer,
                     "Bonjour monde", "ï½Ÿmrk_case_modifier_Cï½  bonï¿­ jï¿­ our monï¿­ de");
  test_tok_and_detok(tokenizer,
                     "BONJOUR MONDE", "ï½Ÿmrk_begin_case_region_Uï½  bonï¿­ jï¿­ our ï½Ÿmrk_end_case_region_Uï½  ï½Ÿmrk_begin_case_region_Uï½  monï¿­ de ï½Ÿmrk_end_case_region_Uï½ ");
  test_tok_and_detok(tokenizer,
                     "BONJOUR monde", "ï½Ÿmrk_begin_case_region_Uï½  bonï¿­ jï¿­ our ï½Ÿmrk_end_case_region_Uï½  monï¿­ de");
}

TEST(TokenizerTest, CaseMarkupWithMixedScripts) {
  Tokenizer::Options options;
  options.case_markup = true;
  options.joiner_annotate = true;
  test_tok_and_detok(options,
                     "ã€Œæˆ‘ã€…ã¯ã€ã†ã¾ãã‚„ã£ã¦ã„ã‚‹ã“ã¨ã‚’è¨¼æ˜ã—ãŸã€ã¨ãƒ’ãƒ§ãƒ³CEOã¯è©±ã™ã€‚",
                     "ã€Œï¿­ æˆ‘ã€…ã¯ ï¿­ã€ï¿­ ã†ã¾ãã‚„ã£ã¦ã„ã‚‹ã“ã¨ã‚’è¨¼æ˜ã—ãŸ ï¿­ã€ï¿­ ã¨ãƒ’ãƒ§ãƒ³ï¿­ ï½Ÿmrk_begin_case_region_Uï½  ceoï¿­ ï½Ÿmrk_end_case_region_Uï½  ã¯è©±ã™ ï¿­ã€‚");
}

TEST(TokenizerTest, CaseMarkupDetokenizationWithPlaceholders) {
  Tokenizer::Options options;
  options.case_markup = true;
  test_detok(options, "ï½Ÿmrk_case_modifier_Cï½  ï½Ÿabcï½ ", "ï½Ÿabcï½ ");
  test_detok(options, "ï½Ÿmrk_begin_case_region_Uï½  ï½Ÿabcï½  ï½Ÿmrk_end_case_region_Uï½ ", "ï½Ÿabcï½ ");
}

TEST(TokenizerTest, CaseMarkupDetokenizationWithMissingModifiedToken) {
  Tokenizer::Options options;
  options.case_markup = true;
  test_detok(options, "hello ï½Ÿmrk_case_modifier_Cï½ ", "hello");
  test_detok(options, "ï½Ÿmrk_case_modifier_Cï½  ï½Ÿmrk_case_modifier_Cï½  hello", "Hello");
  test_detok(options, "ï½Ÿmrk_case_modifier_Cï½  ï½Ÿmrk_begin_case_region_Uï½  hello ï½Ÿmrk_end_case_region_Uï½ ", "HELLO");
}

TEST(TokenizerTest, CaseMarkupDetokenizationWithMissingRegionMarker) {
  Tokenizer::Options options;
  options.case_markup = true;
  test_detok(options, "ï½Ÿmrk_begin_case_region_Uï½  hello", "HELLO");
  test_detok(options,
             "ï½Ÿmrk_begin_case_region_Uï½  hello ï½Ÿmrk_case_modifier_Cï½  world", "HELLO World");
  test_detok(options,
             "ï½Ÿmrk_end_case_region_Uï½  hello ï½Ÿmrk_case_modifier_Cï½  world", "hello World");
}

TEST(TokenizerTest, CaseMarkupDetokenizationWithNestedMarkers) {
  Tokenizer::Options options;
  options.case_markup = true;
  test_detok(options,
             "ï½Ÿmrk_begin_case_region_Uï½  ï½Ÿmrk_case_modifier_Cï½  hello world ï½Ÿmrk_end_case_region_Uï½ ", "Hello WORLD");
  test_detok(options,
             "ï½Ÿmrk_begin_case_region_Uï½  hello ï½Ÿmrk_case_modifier_Cï½  ï½Ÿmrk_end_case_region_Uï½  world", "HELLO world");
}

TEST(TokenizerTest, CaseMarkupWithLocaleEl) {
  Tokenizer::Options options;
  options.case_markup = true;
  options.soft_case_regions = true;
  options.lang = "el";
  test_tok(options,
           "Î£Î™Î“ÎœÎ‘ Î¤Î•Î›Î™ÎšÎŸÎ£",
           "ï½Ÿmrk_begin_case_region_Uï½  ÏƒÎ¹Î³Î¼Î± Ï„ÎµÎ»Î¹ÎºÎ¿Ï‚ ï½Ÿmrk_end_case_region_Uï½ ");
  test_detok(options,
             "ï½Ÿmrk_begin_case_region_Uï½  Ï„Î·Î½ Î¬Î½Î¿Î¹Î¾Î· , Î±Ï€ÏÎ¯Î»Î¹Î¿ Î® Î¼Î¬Î¹Î¿ , Î¸Î± ÎºÎ±Ï„Î±Î½Î±Î»ÏÏƒÏ‰ Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎµÏ‚ Ï€Î¿ÏƒÏŒÏ„Î·Ï„ÎµÏ‚ Ï€ÏÏ‰Ï„ÎµÎÎ½Î·Ï‚ ï½Ÿmrk_end_case_region_Uï½ ",
             "Î¤Î—Î Î‘ÎÎŸÎ™ÎÎ— , Î‘Î Î¡Î™Î›Î™ÎŸ Î—Ì ÎœÎ‘ÎªÎŸ , Î˜Î‘ ÎšÎ‘Î¤Î‘ÎÎ‘Î›Î©Î£Î© ÎœÎ•Î“Î‘Î›Î¥Î¤Î•Î¡Î•Î£ Î ÎŸÎ£ÎŸÎ¤Î—Î¤Î•Î£ Î Î¡Î©Î¤Î•ÎªÎÎ—Î£");
}

TEST(TokenizerTest, CaseMarkupWithLocaleNl) {
  Tokenizer::Options options;
  options.case_markup = true;
  options.lang = "nl";
  test_detok(options, "ï½Ÿmrk_case_modifier_Cï½  ijssel", "IJssel");
}

TEST(TokenizerTest, SegmentCase) {
  Tokenizer::Options options;
  options.case_feature = true;
  options.joiner_annotate = true;
  options.segment_case = true;
  test_tok_and_detok(options, "WiFi", "wiï¿­ï¿¨C fiï¿¨C");
}

TEST(TokenizerTest, SegmentNumbers) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  options.joiner_annotate = true;
  options.segment_numbers = true;
  test_tok_and_detok(options,
                     "1984 mille neuf cent quatrevingt-quatre",
                     "1ï¿­ 9ï¿­ 8ï¿­ 4 mille neuf cent quatrevingt ï¿­-ï¿­ quatre");
}

TEST(TokenizerTest, SegmentAlphabet) {
  Tokenizer::Options options;
  options.joiner_annotate = true;
  options.segment_alphabet = {"Han"};
  test_tok_and_detok(options, "rawĞ‘", "rawĞ‘");
  test_tok(options,
           "æœ‰å…¥è²å˜…å”è©±å¾€å¾€æœ‰é™½å…¥å°è½‰ï¼Œå³ä¿‚å…¥è²éŸ»å°¾åŒé¼»éŸ³éŸ»å°¾å¯ä»¥è½‰åŒ–ã€‚æ¯”å¦‚ç²µèªå˜…ã€ŒæŠŒã€ï¼ˆdamï¼‰ã€Œæ¼ã€ï¼ˆdapï¼‰ï¼Œæ„æ€æ¥è¿‘ï¼Œæ„å‘³å¾®å¦™ï¼Œå€åˆ¥åœ¨æ–¼-måŒ-på˜…è½‰æ›ã€‚",
           "æœ‰ï¿­ å…¥ï¿­ è²ï¿­ å˜…ï¿­ å”ï¿­ è©±ï¿­ å¾€ï¿­ å¾€ï¿­ æœ‰ï¿­ é™½ï¿­ å…¥ï¿­ å°ï¿­ è½‰ ï¿­ï¼Œï¿­ å³ï¿­ ä¿‚ï¿­ å…¥ï¿­ è²ï¿­ éŸ»ï¿­ å°¾ï¿­ åŒï¿­ é¼»ï¿­ éŸ³ï¿­ éŸ»ï¿­ å°¾ï¿­ å¯ï¿­ ä»¥ï¿­ è½‰ï¿­ åŒ– ï¿­ã€‚ï¿­ æ¯”ï¿­ å¦‚ï¿­ ç²µï¿­ èªï¿­ å˜… ï¿­ã€Œï¿­ æŠŒ ï¿­ã€ ï¿­ï¼ˆï¿­ dam ï¿­ï¼‰ ï¿­ã€Œï¿­ æ¼ ï¿­ã€ ï¿­ï¼ˆï¿­ dap ï¿­ï¼‰ ï¿­ï¼Œï¿­ æ„ï¿­ æ€ï¿­ æ¥ï¿­ è¿‘ ï¿­ï¼Œï¿­ æ„ï¿­ å‘³ï¿­ å¾®ï¿­ å¦™ ï¿­ï¼Œï¿­ å€ï¿­ åˆ¥ï¿­ åœ¨ï¿­ æ–¼-måŒ-på˜…ï¿­ è½‰ï¿­ æ› ï¿­ã€‚");
}

// Checking backward compatibility with the "Kanbun" and "Kangxi" alphabets that are not
// included in ICU list of Unicode script aliases.
TEST(TokenizerTest, SegmentAlphabetKangxi) {
  Tokenizer::Options options;
  options.segment_alphabet = {"Kangxi"};
  test_tok(options, "12â¼€â¼", "12 â¼€ â¼");
}
TEST(TokenizerTest, SegmentAlphabetKanbun) {
  Tokenizer::Options options;
  options.segment_alphabet = {"Kanbun"};
  test_tok(options, "12ã†™ã†š", "12 ã†™ ã†š");
}

TEST(TokenizerTest, SegmentAlphabetChange) {
  Tokenizer::Options options;
  options.segment_alphabet_change = true;
  test_tok(options, "rawĞ‘", "raw Ğ‘");
}

TEST(TokenizerTest, SegmentAlphabetChangeCommonScript) {
  Tokenizer::Options options;
  options.segment_alphabet_change = true;
  // Character ãƒ¼ can appear in both Hiragana and Katakana and should not be segmented when
  // appearing in these contexts. See https://github.com/OpenNMT/Tokenizer/issues/210.
  test_tok(options, "ã€Œã‚­ãƒ£ãƒ³ãƒˆãƒ»ãƒã‚¤ãƒ»ãƒŸãƒ¼ãƒ»ãƒ©ãƒ´ã€", "ã€Œ ã‚­ãƒ£ãƒ³ãƒˆ ãƒ» ãƒã‚¤ ãƒ» ãƒŸãƒ¼ ãƒ» ãƒ©ãƒ´ ã€");
}

TEST(TokenizerTest, SegmentAlphabetChangeIsolatedMarks) {
  Tokenizer::Options options;
  options.segment_alphabet_change = true;
  options.allow_isolated_marks = true;
  options.joiner_annotate = true;
  test_tok(options, "abcà¦¼", "abc ï¿­à¦¼");
  test_tok(options, "8à§‡", "8 ï¿­à§‡");
  test_tok(options, "ĞµÌˆ", "ĞµÌˆ");  // combining mark with inherited script.

  options.preserve_segmented_tokens = true;
  test_tok(options, "abcà¦¼", "abc ï¿­ à¦¼");
}

TEST(TokenizerTest, PreserveSegmentedNumbers) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  options.joiner_annotate = true;
  options.segment_numbers = true;
  options.preserve_segmented_tokens = true;
  test_tok_and_detok(options, "1234", "1 ï¿­ 2 ï¿­ 3 ï¿­ 4");
}

TEST(TokenizerTest, PreserveSegmentAlphabetChange) {
  Tokenizer::Options options;
  options.joiner_annotate = true;
  options.segment_alphabet_change = true;
  options.preserve_segmented_tokens = true;
  test_tok_and_detok(options, "rawĞ‘", "raw ï¿­ Ğ‘");
}

TEST(TokenizerTest, PreserveSegmentAlphabet) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  options.joiner_annotate = true;
  options.segment_alphabet = {"Han"};
  options.segment_alphabet_change = true;
  options.preserve_segmented_tokens = true;
  test_tok_and_detok(options, "æ¸¬è©¦abc", "æ¸¬ ï¿­ è©¦ ï¿­ abc");
}

TEST(TokenizerTest, PreserveSegmentCase) {
  Tokenizer::Options options;
  options.joiner_annotate = true;
  options.segment_case = true;
  options.preserve_segmented_tokens = true;
  test_tok_and_detok(options, "WiFi", "Wi ï¿­ Fi");
}

TEST(TokenizerTest, PreserveSegmentCaseBPE) {
  Tokenizer::Options options;
  options.joiner_annotate = true;
  options.segment_case = true;
  options.preserve_segmented_tokens = true;
  Tokenizer tokenizer(options, std::make_shared<BPE>(get_data("bpe-models/fr500")));
  test_tok_and_detok(tokenizer, "BonjourMonde", "Bï¿­ onï¿­ jouï¿­ r ï¿­ Mï¿­ onï¿­ de");
  test_tok_and_detok(tokenizer, "aB", "a ï¿­ B");
}

TEST(TokenizerTest, BPE) {
  Tokenizer::Options options;
  options.joiner_annotate = true;
  Tokenizer tokenizer(options, std::make_shared<BPE>(get_data("bpe-models/testcode.v0.1")));
  test_tok_and_detok(tokenizer,
                     "abcdimprovementè”åˆå›½",
                     "aï¿­ bï¿­ cï¿­ dï¿­ imprï¿­ ovemenï¿­ tï¿­ è”åˆï¿­ å›½");
}

TEST(TokenizerTest, BPEModePrefix) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  Tokenizer tokenizer(options, std::make_shared<BPE>(get_data("bpe-models/codes_prefix.fr")));
  test_tok(tokenizer,
           "Seulement seulement il vais nonseulement seulementnon Ã  Verdun",
           "S e u lement seulement il v ais n on se u lement seulement n on Ã  V er d un");
}

TEST(TokenizerTest, BPEModeNofix) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  Tokenizer tokenizer(options, std::make_shared<BPE>(get_data("bpe-models/codes_nofix.fr")));
  test_tok(tokenizer,
           "Seulement seulement il vais nonseulement seulementnon Ã  Verdun",
           "S e u lement seulement il v ais n on seulement seulement n on Ã  V er d un");
}

TEST(TokenizerTest, BPEModeBoth) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  Tokenizer tokenizer(options, std::make_shared<BPE>(get_data("bpe-models/codes_bothfix.fr")));
  test_tok(tokenizer,
           "Seulement seulement il vais nonseulement seulementnon Ã  Verdun",
           "S eu lement seulement il va is n on s eu lement seu l emen t n on Ã  V er du n");
}

TEST(TokenizerTest, BPECaseInsensitive) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  Tokenizer tokenizer(options, std::make_shared<BPE>(get_data("bpe-models/codes_suffix_case_insensitive.fr")));
  test_tok(tokenizer,
           "Seulement seulement il vais nonseulement seulementnon Ã  Verdun",
           "Seulement seulement il va is n on seulement seu l em ent n on Ã  Ver d un");
}

TEST(TokenizerTest, BPEDropout) {
  Tokenizer tokenizer({}, std::make_shared<BPE>(get_data("bpe-models/codes_suffix_case_insensitive.fr"), 1.0));
  test_tok(tokenizer, "seulement", "s e u l e m e n t");
  test_tok(tokenizer, "seulement", "seulement", /*detokenize=*/false, /*training=*/false);
}

TEST(TokenizerTest, BPEVocabularyWithTrailingJoiner) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Space;
  options.joiner_annotate = true;
  options.support_prior_joiners = true;
  auto bpe = std::make_shared<BPE>(get_data("bpe-models/bpe_code.v0.2"));
  bpe->set_vocabulary({"welï¿­"});
  Tokenizer tokenizer(options, bpe);
  test_tok(tokenizer, "welï¿­ le", "welï¿­ lï¿­ e");
  test_tok(tokenizer, "wel le", "wï¿­ eï¿­ l lï¿­ e");
}

TEST(TokenizerTest, BPEVocabularyWithLeadingJoiner) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  options.joiner_annotate = true;
  auto bpe = std::make_shared<BPE>(get_data("bpe-models/bpe_code.v0.2"));
  bpe->set_vocabulary({"ï¿­10"});
  Tokenizer tokenizer(options, bpe);
  test_tok(tokenizer, "A10", "A ï¿­10");
  test_tok(tokenizer, "A100", "A ï¿­1ï¿­ 0ï¿­ 0");
}

TEST(TokenizerTest, BPEVocabularyWithPreservedTokens) {
  Tokenizer::Options options;
  options.joiner_annotate = true;
  options.joiner = "ï¿­";

  BPE bpe(get_data("bpe-models/bpe_code.v0.2"));
  bpe.set_vocabulary({"wel"}, &options);

  Token token("welle");
  token.preserve = true;
  token.join_right = true;
  auto subwords = bpe.encode_and_annotate(token);

  ASSERT_EQ(subwords.size(), 5);

  EXPECT_EQ(subwords[0].surface, "w");
  EXPECT_TRUE(subwords[0].join_right);
  EXPECT_FALSE(subwords[0].preserve);

  EXPECT_EQ(subwords[1].surface, "e");
  EXPECT_TRUE(subwords[1].join_right);
  EXPECT_FALSE(subwords[1].preserve);

  EXPECT_EQ(subwords[2].surface, "l");
  EXPECT_TRUE(subwords[2].join_right);
  EXPECT_FALSE(subwords[2].preserve);
}

TEST(TokenizerTest, BPEVocabularyWithSpacer) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Space;
  options.spacer_annotate = true;

  auto bpe = std::make_shared<BPE>(get_data("bpe-models/bpe_code.v0.2"));
  bpe->set_vocabulary({"â–wel"}, &options);
  Tokenizer tokenizer(options, bpe);

  test_tok(tokenizer, "die welle", "d i e â–wel l e");
}

TEST(TokenizerTest, BPEWithoutVocabulary) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Space;
  options.joiner = "@@";
  options.joiner_annotate = true;
  Tokenizer tokenizer(options, std::make_shared<BPE>(get_data("bpe-models/bpe_code.v0.2")));
  test_tok(tokenizer, "Oliver GrÃ¼n , welle", "Oliver GrÃ¼n , welle");
}

TEST(TokenizerTest, BPEWithVocabulary) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Space;
  options.joiner = "@@";
  options.joiner_annotate = true;
  auto bpe = std::make_shared<BPE>(get_data("bpe-models/bpe_code.v0.2"));
  bpe->load_vocabulary(get_data("bpe-models/vocab.en"), 50, &options);
  Tokenizer tokenizer(options, bpe);
  test_tok(tokenizer, "Oliver GrÃ¼n , welle", "Oliver Gr@@ Ã¼@@ n , wel@@ le");
}

TEST(TokenizerTest, BPEWithVocabularyTabSeparated) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Space;
  options.joiner = "@@";
  options.joiner_annotate = true;
  auto bpe = std::make_shared<BPE>(get_data("bpe-models/bpe_code.v0.2"));
  bpe->load_vocabulary(get_data("bpe-models/vocab.en.tab"), 50, &options);
  Tokenizer tokenizer(options, bpe);
  test_tok(tokenizer, "Oliver GrÃ¼n , welle", "Oliver Gr@@ Ã¼@@ n , wel@@ le");
}

TEST(TokenizerTest, SentencePiece) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::None;
  Tokenizer tokenizer(options, std::make_shared<SentencePiece>(get_data("sp-models/sp.model")));
  test_tok_and_detok(tokenizer,
                     "The two shows, called Desire and Secrets, will be one-hour prime-time shows.",
                     "The â–two â–shows , â–called â–De si re â–and â–S e c re t s , â–will â–be â–one - hour â–prime - time â–shows .");
}

TEST(TokenizerTest, SentencePieceWithJoinersAndPlaceholders) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::None;
  options.joiner_annotate = true;
  Tokenizer tokenizer(options, std::make_shared<SentencePiece>(get_data("sp-models/sp.model")));
  test_tok_and_detok(tokenizer,
                     "The two shows, called ï½ŸDesireï½  and Secrets, will be one-hour prime-time shows.",
                     "The two shows ï¿­, called ï½ŸDesireï½  and S ï¿­e ï¿­c ï¿­re ï¿­t ï¿­s ï¿­, will be one ï¿­- ï¿­hour prime ï¿­- ï¿­time shows ï¿­.");
  test_tok_and_detok(tokenizer,
                     "The two shows, calledï½ŸDesireï½ and Secrets, will be one-hour prime-time shows.",
                     "The two shows ï¿­, calledï¿­ ï½ŸDesireï½ ï¿­ and S ï¿­e ï¿­c ï¿­re ï¿­t ï¿­s ï¿­, will be one ï¿­- ï¿­hour prime ï¿­- ï¿­time shows ï¿­.");
}

TEST(TokenizerTest, SentencePieceWithJoinersAndPreservePlaceholders) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::None;
  options.joiner_annotate = true;
  options.preserve_placeholders = true;
  Tokenizer tokenizer(options, std::make_shared<SentencePiece>(get_data("sp-models/sp.model")));
  test_tok_and_detok(tokenizer,
                     "The two shows, calledï½ŸDesireï½ and Secrets, will be one-hour prime-time shows.",
                     "The two shows ï¿­, calledï¿­ ï½ŸDesireï½  ï¿­ and S ï¿­e ï¿­c ï¿­re ï¿­t ï¿­s ï¿­, will be one ï¿­- ï¿­hour prime ï¿­- ï¿­time shows ï¿­.");
}

TEST(TokenizerTest, SentencePieceSubwordRegularization) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::None;
  Tokenizer tokenizer(options, std::make_shared<SentencePiece>(get_data("sp-models/sp_regularization.model"), 1, 0.1));
  test_tok_and_detok(tokenizer,
                     "The two shows, called Desire and Secrets, will be one-hour prime-time shows.",
                     "â–The â– two â–show s , â–call ed â–De si re â– and â–Sec re t s , â–w ill â–be â–one - h our â– pri me - t im e â–show s .");
}

TEST(TokenizerTest, SentencePieceAlt) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::None;
  Tokenizer tokenizer(options, std::make_shared<SentencePiece>(get_data("sp-models/wmtende.model")));
  test_tok_and_detok(tokenizer,
                     "Bamford is appealing the sentence and has been granted bail of 50,000 baht.",
                     "â–Ba m ford â–is â–appealing â–the â–sentence â–and â–has â–been â–granted â–bail â–of â– 50,000 â–ba ht .");
}

TEST(TokenizerTest, SentencePieceLeadingSpacer) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::None;
  Tokenizer tokenizer(options, std::make_shared<SentencePiece>(get_data("sp-models/wmtende.model")));
  test_tok_and_detok(tokenizer, "Experts", "â– Expert s");
  test_tok_and_detok(tokenizer, "Expert", "â– Expert");
}

TEST(TokenizerTest, SentencePieceWithJoiners) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::None;
  options.joiner_annotate = true;
  Tokenizer tokenizer(options, std::make_shared<SentencePiece>(get_data("sp-models/sp.model")));
  test_tok_and_detok(tokenizer,
                     "The two shows, called Desire and Secrets, will be one-hour prime-time shows.",
                     "The two shows ï¿­, called De ï¿­si ï¿­re and S ï¿­e ï¿­c ï¿­re ï¿­t ï¿­s ï¿­, will be one ï¿­- ï¿­hour prime ï¿­- ï¿­time shows ï¿­.");
}

TEST(TokenizerTest, SentencePieceAggressive) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  Tokenizer tokenizer(options, std::make_shared<SentencePiece>(get_data("sp-models/wmtende.model")));
  test_tok(tokenizer,
           "Bamford is appealing the sentence and has been granted bail of 50,000 baht.",
           "Ba m ford is appealing the sentence and has been granted bail of 50 , 000 ba ht .");
}

TEST(TokenizerTest, SentencePieceAggressiveAndSpacers) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  options.spacer_annotate = true;
  Tokenizer tokenizer(options, std::make_shared<SentencePiece>(get_data("sp-models/sp.model")));
  test_tok_and_detok(tokenizer,
                     "The two shows, called Desire and Secrets, will be one-hour prime-time shows.",
                     "The â–t wo â–s how s , â–called â–D es ir e â–and â–Se c re t s , â–will â–be â–one - hour â–p rime - time â–s how s .");
}

TEST(TokenizerTest, SentencePieceAggressiveAndJoiners) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  options.joiner_annotate = true;
  Tokenizer tokenizer(options, std::make_shared<SentencePiece>(get_data("sp-models/sp.model")));
  test_tok_and_detok(tokenizer,
                     "The two shows, called Desire and Secrets, will be one-hour prime-time shows.",
                     "The t ï¿­wo s ï¿­how ï¿­s ï¿­, called D ï¿­es ï¿­ir ï¿­e and Se ï¿­c ï¿­re ï¿­t ï¿­s ï¿­, will be one ï¿­-ï¿­ hour p ï¿­rime ï¿­-ï¿­ time s ï¿­how ï¿­s ï¿­.");
}

TEST(TokenizerTest, SentencePieceIsolatedSpacer) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::None;
  options.preserve_placeholders = true;
  Tokenizer tokenizer(options, std::make_shared<SentencePiece>(get_data("sp-models/wmtende.model")));
  test_tok(tokenizer, "a crumpled sofa", "â–a â– cru mpl ed â–sofa");
  test_tok(tokenizer, "a ï½Ÿphï½ crumpled sofa", "â–a â– ï½Ÿphï½  cru mpl ed â–sofa");
}

TEST(TokenizerTest, SentencePieceIsolatedSpacerAndJoinerAnnotate) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::None;
  options.joiner_annotate = true;
  options.preserve_placeholders = true;
  Tokenizer tokenizer(options, std::make_shared<SentencePiece>(get_data("sp-models/wmtende.model")));
  test_tok(tokenizer, "a crumpled sofa", "a cru ï¿­mpl ï¿­ed sofa");
  test_tok(tokenizer, "a ï½Ÿphï½ crumpled sofa", "a ï½Ÿphï½  ï¿­ cru ï¿­mpl ï¿­ed sofa");
  test_tok(tokenizer, "ï½Ÿphï½ ,", "ï½Ÿphï½  ï¿­ ,");
}

TEST(TokenizerTest, SentencePieceAggressiveIsolatedSpacerAndJoinerAnnotate) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  options.joiner_annotate = true;
  Tokenizer tokenizer(options, std::make_shared<SentencePiece>(get_data("sp-models/wmtende.model")));
  test_tok(tokenizer, "depending on its temperature.", "depending on its temperature ï¿­.");
}

TEST(TokenizerTest, TokenInterface) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Aggressive;
  options.joiner_annotate = true;
  options.case_markup = true;
  Tokenizer tokenizer(options);
  const std::string text = "Hello world!";
  std::vector<Token> tokens;
  tokenizer.tokenize(text, tokens);
  EXPECT_EQ(tokens[0].surface, "hello");
  EXPECT_EQ(tokens[1].surface, "world");
  EXPECT_EQ(tokens[2].surface, "!");
  EXPECT_EQ(tokenizer.detokenize(tokens), text);
}

int main(int argc, char *argv[]) {
  testing::InitGoogleTest(&argc, argv);
  assert(argc == 2);
  data_dir = argv[1];
  return RUN_ALL_TESTS();
}
