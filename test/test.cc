#include <gtest/gtest.h>

#include <onmt/BPE.h>
#include <onmt/SentencePiece.h>
#include <onmt/Tokenizer.h>
#include <onmt/SpaceTokenizer.h>

using namespace onmt;

static std::string data_dir;

static std::string get_data(const std::string& path) {
  return data_dir + "/" + path;
}

static void test_tok(ITokenizer& tokenizer,
                     const std::string& in,
                     const std::string& expected,
                     bool detokenize = false) {
  std::vector<std::string> tokens;
  std::vector<std::vector<std::string>> features;
  tokenizer.tokenize(in, tokens, features);
  EXPECT_EQ(SpaceTokenizer::get_instance().detokenize(tokens, features), expected);
  if (detokenize) {
    EXPECT_EQ(tokenizer.detokenize(tokens, features), in);
  }
}

static void test_tok(ITokenizer& tokenizer,
                     const std::string& in,
                     const std::vector<std::string>& expected,
                     bool detokenize = false) {
  std::vector<std::string> tokens;
  tokenizer.tokenize(in, tokens);
  ASSERT_EQ(tokens.size(), expected.size());
  for (size_t i = 0; i < tokens.size(); ++i) {
    EXPECT_EQ(tokens[i], expected[i])  << "Unexpected token mismatch at index " << i;
  }
  if (detokenize) {
    auto text = tokenizer.detokenize(tokens);
    EXPECT_EQ(text, in);
  }
}

static void test_detok(ITokenizer& tokenizer, const std::string& in, const std::string& expected) {
  std::vector<std::string> tokens;
  onmt::SpaceTokenizer::get_instance().tokenize(in, tokens);
  auto detok = tokenizer.detokenize(tokens);
  EXPECT_EQ(detok, expected);
}

static void test_tok_alphabet(ITokenizer& tokenizer,
                              const std::string& in,
                              const std::string& expected,
                              const std::unordered_map<std::string, size_t>& expected_alphabets) {
  std::vector<std::string> words;
  std::vector<std::vector<std::string> > features;
  std::unordered_map<std::string, size_t> alphabets;

  tokenizer.tokenize(in, words, features, alphabets);

  std::string output;
  for (size_t i = 0; i < words.size(); ++i)
  {
    if (i > 0)
      output += " ";
    output += words[i];
  }

  EXPECT_EQ(output, expected);

  for (const auto& it: expected_alphabets) {
    ASSERT_TRUE(alphabets.find(it.first) != alphabets.end());
    EXPECT_EQ(alphabets[it.first], it.second);
  }
}

static void test_tok_and_detok(ITokenizer& tokenizer,
                               const std::string& in,
                               const std::string& expected) {
  return test_tok(tokenizer, in, expected, true);
}

TEST(TokenizerTest, DetokenizeWithRanges) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative);
  Ranges ranges;
  tokenizer.detokenize({"Helï¿­", "lo", "ï½Ÿmrk_case_modifier_Cï½ ", "w", "ï¿­", "orld", "ï¿­!"}, ranges);
  // Result: Hello World!
  ASSERT_EQ(ranges.size(), 5);
  EXPECT_EQ(ranges[0], (std::pair<size_t, size_t>(0, 2)));
  EXPECT_EQ(ranges[1], (std::pair<size_t, size_t>(3, 4)));
  EXPECT_EQ(ranges[3], (std::pair<size_t, size_t>(6, 6)));
  EXPECT_EQ(ranges[5], (std::pair<size_t, size_t>(7, 10)));
  EXPECT_EQ(ranges[6], (std::pair<size_t, size_t>(11, 11)));
}

TEST(TokenizerTest, DetokenizeWithMergedRanges) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative);
  Ranges ranges;
  tokenizer.detokenize({"Helï¿­", "lo", "wï¿­", "orld", "ï¿­!"}, ranges, true);
  // Result: Hello World!
  ASSERT_EQ(ranges.size(), 5);
  EXPECT_EQ(ranges[0], (std::pair<size_t, size_t>(0, 4)));
  EXPECT_EQ(ranges[1], (std::pair<size_t, size_t>(0, 4)));
  EXPECT_EQ(ranges[2], (std::pair<size_t, size_t>(6, 10)));
  EXPECT_EQ(ranges[3], (std::pair<size_t, size_t>(6, 10)));
  EXPECT_EQ(ranges[4], (std::pair<size_t, size_t>(11, 11)));
}

TEST(TokenizerTest, DetokenizeWithMergedRangesPlaceholders) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative);
  Ranges ranges;
  tokenizer.detokenize({"ï½Ÿaï½ ï¿­", "b", "ï¿­ï½Ÿcï½ "}, ranges, true);
  // Result: ï½Ÿaï½ bï½Ÿcï½ 
  ASSERT_EQ(ranges.size(), 3);
  EXPECT_EQ(ranges[0], (std::pair<size_t, size_t>(0, 6)));
  EXPECT_EQ(ranges[1], (std::pair<size_t, size_t>(7, 7)));
  EXPECT_EQ(ranges[2], (std::pair<size_t, size_t>(8, 14)));
}

TEST(TokenizerTest, BasicConservative) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative);
  test_tok(tokenizer,
           "Your Hardware-Enablement Stack (HWE) is supported until April 2019.",
           "Your Hardware-Enablement Stack ( HWE ) is supported until April 2019 .");
}
TEST(TokenizerTest, ConservativeEmpty) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative);
  test_tok(tokenizer, "", "");
}

TEST(TokenizerTest, None) {
  Tokenizer tokenizer(Tokenizer::Mode::None);
  test_tok(tokenizer, "Hello World!", "Hello World!");
}

TEST(TokenizerTest, NoneWithPlaceholders1) {
  Tokenizer tokenizer(Tokenizer::Mode::None, Tokenizer::Flags::JoinerAnnotate);
  test_tok(tokenizer, "Hello:ï½ŸWorldï½ !", "Hello:ï¿­ ï½ŸWorldï½ ï¿­ !");
}

TEST(TokenizerTest, NoneWithPlaceholders2) {
  Tokenizer tokenizer(Tokenizer::Mode::None, Tokenizer::Flags::JoinerAnnotate |
                      Tokenizer::Flags::PreservePlaceholders);
  test_tok(tokenizer, "Hello:ï½ŸWorldï½ !", "Hello:ï¿­ ï½ŸWorldï½  ï¿­ !");
}

TEST(TokenizerTest, NonePlaceholderSpacesEscape) {
  Tokenizer tokenizer(Tokenizer::Mode::None, Tokenizer::Flags::None);
  test_tok(tokenizer, "ï½Ÿa b cï½ ", "ï½Ÿaï¼…0020bï¼…0020cï½ ");
}

TEST(TokenizerTest, NonePlaceholderSpacesNoEscape) {
  Tokenizer tokenizer(Tokenizer::Mode::None, Tokenizer::Flags::NoSubstitution);
  test_tok(tokenizer, "ï½Ÿa b cï½ ", "ï½Ÿa b cï½ ");
}

TEST(TokenizerTest, PreserveTokensInNoneMode) {
  Tokenizer tokenizer(Tokenizer::Mode::None,
                      Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::PreserveSegmentedTokens);
  test_tok(tokenizer, "Helloï½ŸWorldï½ !", "Hello ï¿­ ï½ŸWorldï½  ï¿­ !");
}

TEST(TokenizerTest, BasicSpace) {
  Tokenizer tokenizer(Tokenizer::Mode::Space);
  test_tok(tokenizer,
           "49th meeting Social and human rights questions: human rights [14 (g)]",
           "49th meeting Social and human rights questions: human rights [14 (g)]");
}
TEST(TokenizerTest, SpaceEmpty) {
  Tokenizer tokenizer(Tokenizer::Mode::Space);
  test_tok(tokenizer, "", "");
}
TEST(TokenizerTest, SpaceSingle) {
  Tokenizer tokenizer(Tokenizer::Mode::Space);
  test_tok(tokenizer, "Hello", "Hello");
}
TEST(TokenizerTest, SpaceLeading) {
  Tokenizer tokenizer(Tokenizer::Mode::Space);
  test_tok(tokenizer, " Hello", "Hello");
}
TEST(TokenizerTest, SpaceTrailing) {
  Tokenizer tokenizer(Tokenizer::Mode::Space);
  test_tok(tokenizer, "Hello ", "Hello");
}
TEST(TokenizerTest, SpaceDuplicated) {
  Tokenizer tokenizer(Tokenizer::Mode::Space);
  test_tok(tokenizer, "  Hello   World ", "Hello World");
}

TEST(TokenizerTest, BasicJoiner) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::JoinerAnnotate);
  test_tok_and_detok(tokenizer,
                     "Isn't it so-greatly working?",
                     "Isn ï¿­'ï¿­ t it so ï¿­-ï¿­ greatly working ï¿­?");
  test_tok_and_detok(tokenizer, "MP3", "MP ï¿­3");
  test_tok_and_detok(tokenizer, "A380", "A ï¿­380");
  test_tok_and_detok(tokenizer, "$1", "$ï¿­ 1");
}

TEST(TokenizerTest, BasicSpaceWithFeatures) {
  Tokenizer tokenizer(Tokenizer::Mode::Space, Tokenizer::Flags::CaseFeature);
  test_tok(tokenizer,
           "49th meeting Social and human rights questions: human rights [14 (g)]",
           "49thï¿¨L meetingï¿¨L socialï¿¨C andï¿¨L humanï¿¨L rightsï¿¨L questions:ï¿¨L humanï¿¨L rightsï¿¨L [14ï¿¨N (g)]ï¿¨L");
}

TEST(TokenizerTest, ProtectedSequence) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::JoinerAnnotate);
  test_tok_and_detok(tokenizer, "ï½Ÿ1,023ï½ km", "ï½Ÿ1,023ï½ ï¿­ km");
  test_tok_and_detok(tokenizer, "Aï½Ÿ380ï½ ", "A ï¿­ï½Ÿ380ï½ ");
  test_tok_and_detok(tokenizer, "ï½Ÿ1,023ï½ ï½Ÿ380ï½ ", "ï½Ÿ1,023ï½ ï¿­ ï½Ÿ380ï½ ");
  test_tok_and_detok(tokenizer, "ï½Ÿ1023ï½ .", "ï½Ÿ1023ï½  ï¿­.");
  test_tok_and_detok(tokenizer, "$ï½Ÿ0.23ï½ ", "$ï¿­ ï½Ÿ0.23ï½ ");
  test_tok_and_detok(tokenizer, "ï½Ÿ0.23ï½ $", "ï½Ÿ0.23ï½  ï¿­$");
  test_tok_and_detok(tokenizer, "ï½ŸUS$ï½ 23", "ï½ŸUS$ï½ ï¿­ 23");
  test_tok_and_detok(tokenizer, "1ï½ŸABCDï½ 0", "1 ï¿­ï½ŸABCDï½ ï¿­ 0");
}

TEST(TokenizerTest, PreserveProtectedSequence) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::PreservePlaceholders);
  test_tok_and_detok(tokenizer, "ï½Ÿ1,023ï½ km", "ï½Ÿ1,023ï½  ï¿­ km");
  test_tok_and_detok(tokenizer, "Aï½Ÿ380ï½ ", "A ï¿­ ï½Ÿ380ï½ ");
  test_tok_and_detok(tokenizer, "ï½Ÿ1,023ï½ ï½Ÿ380ï½ ", "ï½Ÿ1,023ï½  ï¿­ ï½Ÿ380ï½ ");
  test_tok_and_detok(tokenizer, "ï½Ÿ1023ï½ .", "ï½Ÿ1023ï½  ï¿­.");
  test_tok_and_detok(tokenizer, "$ï½Ÿ0.23ï½ ", "$ï¿­ ï½Ÿ0.23ï½ ");
  test_tok_and_detok(tokenizer, "ï½Ÿ0.23ï½ $", "ï½Ÿ0.23ï½  ï¿­$");
  test_tok_and_detok(tokenizer, "ï½ŸUS$ï½ 23", "ï½ŸUS$ï½  ï¿­ 23");
  test_tok_and_detok(tokenizer, "1ï½ŸABCDï½ 0", "1 ï¿­ ï½ŸABCDï½  ï¿­ 0");
}

TEST(TokenizerTest, PreserveProtectedSequenceSpacerAnnotate) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::SpacerAnnotate | Tokenizer::Flags::PreservePlaceholders);
  test_tok_and_detok(tokenizer, "ï½Ÿ1,023ï½ km", "ï½Ÿ1,023ï½  km");
  test_tok_and_detok(tokenizer, "A ï½Ÿ380ï½ ", "A â– ï½Ÿ380ï½ ");
  test_tok_and_detok(tokenizer, "ï½Ÿ1,023ï½ ï½Ÿ380ï½ ", "ï½Ÿ1,023ï½  ï½Ÿ380ï½ ");
  test_tok_and_detok(tokenizer, "ï½Ÿ1,023ï½  ï½Ÿ380ï½ ", "ï½Ÿ1,023ï½  â– ï½Ÿ380ï½ ");
  test_tok_and_detok(tokenizer, "ï½Ÿ1023ï½ .", "ï½Ÿ1023ï½  .");
  test_tok_and_detok(tokenizer, "ï½Ÿ1023ï½  .", "ï½Ÿ1023ï½  â–.");
}

TEST(TokenizerTest, ProtectedSequenceAggressive) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::JoinerAnnotate);
  test_tok_and_detok(tokenizer, "ï½Ÿ1,023ï½ km", "ï½Ÿ1,023ï½ ï¿­ km");
  test_tok_and_detok(tokenizer, "Aï½Ÿ380ï½ ", "A ï¿­ï½Ÿ380ï½ ");
  test_tok_and_detok(tokenizer, "ï½Ÿ1,023ï½ ï½Ÿ380ï½ ", "ï½Ÿ1,023ï½ ï¿­ ï½Ÿ380ï½ ");
  test_tok_and_detok(tokenizer, "ï½Ÿ1023ï½ .", "ï½Ÿ1023ï½  ï¿­.");
  test_tok_and_detok(tokenizer, "$ï½Ÿ0.23ï½ ", "$ï¿­ ï½Ÿ0.23ï½ ");
  test_tok_and_detok(tokenizer, "ï½Ÿ0.23ï½ $", "ï½Ÿ0.23ï½  ï¿­$");
  test_tok_and_detok(tokenizer, "ï½ŸUS$ï½ 23", "ï½ŸUS$ï½ ï¿­ 23");
  test_tok_and_detok(tokenizer, "1ï½ŸABCDï½ 0", "1 ï¿­ï½ŸABCDï½ ï¿­ 0");
}

TEST(TokenizerTest, ProtectedSequenceJoinerNew) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::JoinerNew);
  test_tok_and_detok(tokenizer, "ï½Ÿ1,023ï½ km", "ï½Ÿ1,023ï½  ï¿­ km");
  test_tok_and_detok(tokenizer, "Aï½Ÿ380ï½ ", "A ï¿­ ï½Ÿ380ï½ ");
  test_tok_and_detok(tokenizer, "ï½Ÿ1,023ï½ ï½Ÿ380ï½ ", "ï½Ÿ1,023ï½  ï¿­ ï½Ÿ380ï½ ");
  test_tok_and_detok(tokenizer, "ï½Ÿ1023ï½ .", "ï½Ÿ1023ï½  ï¿­ .");
}

TEST(TokenizerTest, Substitutes) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative);
  test_tok(tokenizer,
           "testï¿­ protectï¿¨, ï¼š, â–, and ï¼… or ï¼ƒ...",
           "test â–  protect â”‚ , : , _ , and % or # . . .");
  test_tok(tokenizer, "ï½Ÿtagï¼švalue with spacesï½ ", "ï½Ÿtagï¼švalueï¼…0020withï¼…0020spacesï½ ");
}

TEST(TokenizerTest, NoSubstitution) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::NoSubstitution);
  test_tok(tokenizer,
           "testï¿­ protectï¿¨, ï¼š, â–, and ï¼… or ï¼ƒ...",
           "test ï¿­ protect ï¿¨ , ï¼š , â– , and ï¼… or ï¼ƒ . . .");
  test_tok(tokenizer, "ï½Ÿtagï¼švalue with spacesï½ ", "ï½Ÿtagï¼švalue with spacesï½ ");
}

TEST(TokenizerTest, ZeroWidthJoiner) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::JoinerAnnotate);
  test_tok_and_detok(tokenizer, "ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦", "ğŸ‘¨ ï¿­â€ ï¿­ğŸ‘© ï¿­â€ ï¿­ğŸ‘¦");
}

TEST(TokenizerTest, CombiningMark) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::JoinerAnnotate);
  test_tok_and_detok(tokenizer,
                     "à¤µà¤°à¥à¤¤à¤®à¤¾à¤¨ à¤²à¤¿à¤ªà¤¿ (à¤¸à¥à¤•à¥à¤°à¤¿à¤ªà¥à¤Ÿ) à¤–à¥‹ à¤œà¤¾à¤à¤—à¥€à¥¤",
                     "à¤µà¤°à¥à¤¤à¤®à¤¾à¤¨ à¤²à¤¿à¤ªà¤¿ (ï¿­ à¤¸à¥à¤•à¥à¤°à¤¿à¤ªà¥à¤Ÿ ï¿­) à¤–à¥‹ à¤œà¤¾à¤à¤—à¥€ ï¿­à¥¤");
}

TEST(TokenizerTest, MarkOnSpace) {
  Tokenizer tokenizer_joiner(Tokenizer::Mode::Conservative, Tokenizer::Flags::JoinerAnnotate);
  test_tok_and_detok(tokenizer_joiner,
                     "b Ì‡c",
                     "b ï¿­ï¼…0020Ì‡ï¿­ c");
  Tokenizer tokenizer_spacer(Tokenizer::Mode::Conservative, Tokenizer::Flags::SpacerAnnotate);
  test_tok_and_detok(tokenizer_spacer,
                     "b Ì‡c",
                     "b ï¼…0020Ì‡ c");
}

TEST(TokenizerTest, MarkOnSpaceNoSubstitution) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::NoSubstitution);
  test_tok(tokenizer, "angles á§1 and á§2", {"angles", "ï¿­ á§ï¿­", "1", "and", "ï¿­ á§ï¿­", "2"}, true);
}

TEST(TokenizerTest, CombiningMarkAfterPlaceholder) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::PreservePlaceholders);
  test_tok_and_detok(tokenizer, "ï½Ÿaï½ ×‚b", "ï½Ÿaï½  ï¿­×‚ï¿­ b");
}

TEST(TokenizerTest, CaseFeature) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::CaseFeature | Tokenizer::Flags::JoinerAnnotate);
  // Note: in C literal strings, \ is escaped by another \.
  test_tok(tokenizer,
           "test \\\\\\\\a Capitalized lowercased UPPERCASÃ‰ miXÃªd - cyrillic-Ğ‘",
           "testï¿¨L \\ï¿¨N ï¿­\\ï¿¨N ï¿­\\ï¿¨N ï¿­\\ï¿­ï¿¨N aï¿¨L capitalizedï¿¨C lowercasedï¿¨L uppercasÃ©ï¿¨U mixÃªdï¿¨M -ï¿¨N cyrillic-Ğ±ï¿¨M");
}

TEST(TokenizerTest, CaseFeatureWithJoinerNew) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      Tokenizer::Flags::CaseFeature
                      | Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::JoinerNew);
  test_tok(tokenizer, "a-b.", "aï¿¨L ï¿­ï¿¨N -ï¿¨N ï¿­ï¿¨N bï¿¨L ï¿­ï¿¨N .ï¿¨N");
}

TEST(TokenizerTest, CaseFeatureWithSpacerNew) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::CaseFeature
                      | Tokenizer::Flags::SpacerAnnotate
                      | Tokenizer::Flags::SpacerNew);
  test_tok(tokenizer, "a b", "aï¿¨L â–ï¿¨N bï¿¨L");
}

TEST(TokenizerTest, CaseMarkupWithJoiners) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::CaseMarkup | Tokenizer::Flags::JoinerAnnotate);
  test_tok_and_detok(tokenizer,
                     "Hello world!", "ï½Ÿmrk_case_modifier_Cï½  hello world ï¿­!");
  test_tok_and_detok(tokenizer,
                     "Hello WORLD!", "ï½Ÿmrk_case_modifier_Cï½  hello ï½Ÿmrk_begin_case_region_Uï½  world ï½Ÿmrk_end_case_region_Uï½  ï¿­!");
  test_tok_and_detok(tokenizer,
                     "HELLO WORLD!", "ï½Ÿmrk_begin_case_region_Uï½  hello ï½Ÿmrk_end_case_region_Uï½  ï½Ÿmrk_begin_case_region_Uï½  world ï½Ÿmrk_end_case_region_Uï½  ï¿­!");
  test_tok_and_detok(tokenizer,
                     "Hello WOrld!", "ï½Ÿmrk_case_modifier_Cï½  hello ï½Ÿmrk_begin_case_region_Uï½  woï¿­ ï½Ÿmrk_end_case_region_Uï½  rld ï¿­!");
  test_tok_and_detok(tokenizer,
                     "hello woRld!", "hello woï¿­ ï½Ÿmrk_case_modifier_Cï½  rld ï¿­!");
  test_tok_and_detok(tokenizer,
                     "hello woRlD!", "hello woï¿­ ï½Ÿmrk_case_modifier_Cï½  rlï¿­ ï½Ÿmrk_case_modifier_Cï½  d ï¿­!");
}

TEST(TokenizerTest, CaseMarkupWithSoftUppercaseRegions) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      Tokenizer::Flags::CaseMarkup
                      | Tokenizer::Flags::SoftCaseRegions
                      | Tokenizer::Flags::JoinerAnnotate);
  test_tok_and_detok(tokenizer,
                     "AA.BB", "ï½Ÿmrk_begin_case_region_Uï½  aa ï¿­.ï¿­ bb ï½Ÿmrk_end_case_region_Uï½ ");
  test_tok_and_detok(tokenizer,
                     "A BC", "ï½Ÿmrk_begin_case_region_Uï½  a bc ï½Ÿmrk_end_case_region_Uï½ ");
  test_tok_and_detok(tokenizer,
                     "AA.", "ï½Ÿmrk_begin_case_region_Uï½  aa ï½Ÿmrk_end_case_region_Uï½  ï¿­.");
  test_tok_and_detok(tokenizer,
                     "A-B/C", "ï½Ÿmrk_begin_case_region_Uï½  a ï¿­-ï¿­ b ï¿­/ï¿­ c ï½Ÿmrk_end_case_region_Uï½ ");
  test_tok_and_detok(tokenizer,
                     "A-B/c", "ï½Ÿmrk_begin_case_region_Uï½  a ï¿­-ï¿­ b ï½Ÿmrk_end_case_region_Uï½  ï¿­/ï¿­ c");
  test_tok_and_detok(tokenizer,
                     "A", "ï½Ÿmrk_case_modifier_Cï½  a");
  test_tok_and_detok(tokenizer,
                     "A-", "ï½Ÿmrk_case_modifier_Cï½  a ï¿­-");
  test_tok_and_detok(tokenizer,
                     "ID: A23X52,",
                     "ï½Ÿmrk_begin_case_region_Uï½  id ï¿­: a ï¿­23ï¿­ x ï¿­52 ï½Ÿmrk_end_case_region_Uï½  ï¿­,");
  test_tok_and_detok(tokenizer,
                     "Show PP-LX-DP",
                     "ï½Ÿmrk_case_modifier_Cï½  show ï½Ÿmrk_begin_case_region_Uï½  pp ï¿­-ï¿­ lx ï¿­-ï¿­ dp ï½Ÿmrk_end_case_region_Uï½ ");
  test_tok_and_detok(tokenizer,
                     "AA ï½ŸBBï½  CC", "ï½Ÿmrk_begin_case_region_Uï½  aa ï½Ÿmrk_end_case_region_Uï½  ï½ŸBBï½  ï½Ÿmrk_begin_case_region_Uï½  cc ï½Ÿmrk_end_case_region_Uï½ ");
}

TEST(TokenizerTest, CaseMarkupWithJoinerNew) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::CaseMarkup
                      | Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::JoinerNew);
  test_detok(tokenizer, "hello ï½Ÿmrk_case_modifier_Cï½  ï¿­ world !", "helloWorld !");
  test_detok(tokenizer, "hello ï½Ÿmrk_case_modifier_Cï½  ï¿­", "hello");
}

TEST(TokenizerTest, CaseMarkupWithSpacers) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::CaseMarkup | Tokenizer::Flags::SpacerAnnotate);
  test_tok_and_detok(tokenizer,
                     "Hello world!", "ï½Ÿmrk_case_modifier_Cï½  hello â–world !");
  test_tok_and_detok(tokenizer,
                     "Hello WORLD!", "ï½Ÿmrk_case_modifier_Cï½  hello ï½Ÿmrk_begin_case_region_Uï½  â–world ï½Ÿmrk_end_case_region_Uï½  !");
  test_tok_and_detok(tokenizer,
                     "Hello WOrld!", "ï½Ÿmrk_case_modifier_Cï½  hello ï½Ÿmrk_begin_case_region_Uï½  â–wo ï½Ÿmrk_end_case_region_Uï½  rld !");
  test_tok_and_detok(tokenizer,
                     "hello woRld!", "hello â–wo ï½Ÿmrk_case_modifier_Cï½  rld !");
}

TEST(TokenizerTest, CaseMarkupWithSpacerNew) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::CaseMarkup
                      | Tokenizer::Flags::SpacerAnnotate
                      | Tokenizer::Flags::SpacerNew);
  test_detok(tokenizer, "hello ï½Ÿmrk_case_modifier_Cï½  â– world !", "hello World!");
  test_detok(tokenizer, "hello ï½Ÿmrk_case_modifier_Cï½  â–", "hello ");
}

TEST(TokenizerTest, CaseMarkupWithBPE) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::CaseMarkup | Tokenizer::Flags::JoinerAnnotate,
                      get_data("bpe-models/codes_suffix_case_insensitive.fr"));
  test_tok_and_detok(tokenizer,
                     "Bonjour monde", "ï½Ÿmrk_case_modifier_Cï½  bonï¿­ jï¿­ our monï¿­ de");
  test_tok_and_detok(tokenizer,
                     "BONJOUR MONDE", "ï½Ÿmrk_begin_case_region_Uï½  bonï¿­ jï¿­ our ï½Ÿmrk_end_case_region_Uï½  ï½Ÿmrk_begin_case_region_Uï½  monï¿­ de ï½Ÿmrk_end_case_region_Uï½ ");
  test_tok_and_detok(tokenizer,
                     "BONJOUR monde", "ï½Ÿmrk_begin_case_region_Uï½  bonï¿­ jï¿­ our ï½Ÿmrk_end_case_region_Uï½  monï¿­ de");
}

TEST(TokenizerTest, CaseMarkupDetokWithPlaceholders) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::CaseMarkup);
  test_detok(tokenizer, "ï½Ÿmrk_case_modifier_Cï½  ï½Ÿabcï½ ", "ï½Ÿabcï½ ");
  test_detok(tokenizer, "ï½Ÿmrk_begin_case_region_Uï½  ï½Ÿabcï½  ï½Ÿmrk_end_case_region_Uï½ ", "ï½Ÿabcï½ ");
}

TEST(TokenizerTest, CaseMarkupDetokMissingModifiedToken) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::CaseMarkup);
  test_detok(tokenizer, "hello ï½Ÿmrk_case_modifier_Cï½ ", "hello");
  test_detok(tokenizer, "ï½Ÿmrk_case_modifier_Cï½  ï½Ÿmrk_case_modifier_Cï½  hello", "Hello");
  test_detok(tokenizer, "ï½Ÿmrk_case_modifier_Cï½  ï½Ÿmrk_begin_case_region_Uï½  hello ï½Ÿmrk_end_case_region_Uï½ ", "HELLO");
}

TEST(TokenizerTest, CaseMarkupDetokMissingRegionMarker) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::CaseMarkup);
  test_detok(tokenizer, "ï½Ÿmrk_begin_case_region_Uï½  hello", "HELLO");
  test_detok(tokenizer,
             "ï½Ÿmrk_begin_case_region_Uï½  hello ï½Ÿmrk_case_modifier_Cï½  world", "HELLO World");
  test_detok(tokenizer,
             "ï½Ÿmrk_end_case_region_Uï½  hello ï½Ÿmrk_case_modifier_Cï½  world", "hello World");
}

TEST(TokenizerTest, CaseMarkupDetokNestedMarkers) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::CaseMarkup);
  test_detok(tokenizer,
             "ï½Ÿmrk_begin_case_region_Uï½  ï½Ÿmrk_case_modifier_Cï½  hello world ï½Ÿmrk_end_case_region_Uï½ ", "Hello WORLD");
  test_detok(tokenizer,
             "ï½Ÿmrk_begin_case_region_Uï½  hello ï½Ÿmrk_case_modifier_Cï½  ï½Ÿmrk_end_case_region_Uï½  world", "HELLO world");
}

TEST(TokenizerTest, SegmentCase) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::CaseFeature | Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::SegmentCase);
  test_tok_and_detok(tokenizer, "WiFi", "wiï¿­ï¿¨C fiï¿¨C");
}

TEST(TokenizerTest, SegmentNumbers) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::SegmentNumbers);
  test_tok_and_detok(tokenizer,
                     "1984 mille neuf cent quatrevingt-quatre",
                     "1ï¿­ 9ï¿­ 8ï¿­ 4 mille neuf cent quatrevingt ï¿­-ï¿­ quatre");
}

TEST(TokenizerTest, SegmentAlphabet) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::JoinerAnnotate);
  tokenizer.add_alphabet_to_segment("Han");
  test_tok_and_detok(tokenizer, "rawĞ‘", "rawĞ‘");
  test_tok(tokenizer,
           "æœ‰å…¥è²å˜…å”è©±å¾€å¾€æœ‰é™½å…¥å°è½‰ï¼Œå³ä¿‚å…¥è²éŸ»å°¾åŒé¼»éŸ³éŸ»å°¾å¯ä»¥è½‰åŒ–ã€‚æ¯”å¦‚ç²µèªå˜…ã€ŒæŠŒã€ï¼ˆdamï¼‰ã€Œæ¼ã€ï¼ˆdapï¼‰ï¼Œæ„æ€æ¥è¿‘ï¼Œæ„å‘³å¾®å¦™ï¼Œå€åˆ¥åœ¨æ–¼-måŒ-på˜…è½‰æ›ã€‚",
           "æœ‰ï¿­ å…¥ï¿­ è²ï¿­ å˜…ï¿­ å”ï¿­ è©±ï¿­ å¾€ï¿­ å¾€ï¿­ æœ‰ï¿­ é™½ï¿­ å…¥ï¿­ å°ï¿­ è½‰ ï¿­ï¼Œï¿­ å³ï¿­ ä¿‚ï¿­ å…¥ï¿­ è²ï¿­ éŸ»ï¿­ å°¾ï¿­ åŒï¿­ é¼»ï¿­ éŸ³ï¿­ éŸ»ï¿­ å°¾ï¿­ å¯ï¿­ ä»¥ï¿­ è½‰ï¿­ åŒ– ï¿­ã€‚ï¿­ æ¯”ï¿­ å¦‚ï¿­ ç²µï¿­ èªï¿­ å˜… ï¿­ã€Œï¿­ æŠŒ ï¿­ã€ ï¿­ï¼ˆï¿­ dam ï¿­ï¼‰ ï¿­ã€Œï¿­ æ¼ ï¿­ã€ ï¿­ï¼ˆï¿­ dap ï¿­ï¼‰ ï¿­ï¼Œï¿­ æ„ï¿­ æ€ï¿­ æ¥ï¿­ è¿‘ ï¿­ï¼Œï¿­ æ„ï¿­ å‘³ï¿­ å¾®ï¿­ å¦™ ï¿­ï¼Œï¿­ å€ï¿­ åˆ¥ï¿­ åœ¨ï¿­ æ–¼-måŒ-på˜…ï¿­ è½‰ï¿­ æ› ï¿­ã€‚");
}

// Checking backward compatibility with the "Kanbun" and "Kangxi" alphabets that are not
// included in ICU list of Unicode script aliases.
TEST(TokenizerTest, SegmentAlphabetKangxi) {
  Tokenizer::Options options;
  options.segment_alphabet = {"Kangxi"};
  Tokenizer tokenizer(options);
  test_tok(tokenizer, "12â¼€â¼", "12 â¼€ â¼");
}
TEST(TokenizerTest, SegmentAlphabetKanbun) {
  Tokenizer::Options options;
  options.segment_alphabet = {"Kanbun"};
  Tokenizer tokenizer(options);
  test_tok(tokenizer, "12ã†™ã†š", "12 ã†™ ã†š");
}

TEST(TokenizerTest, SegmentAlphabetChange) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::SegmentAlphabetChange);
  test_tok(tokenizer, "rawĞ‘", "raw Ğ‘");
}

TEST(TokenizerTest, PreserveSegmentedNumbers) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      Tokenizer::Flags::SegmentNumbers
                      | Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::PreserveSegmentedTokens);
  test_tok_and_detok(tokenizer, "1234", "1 ï¿­ 2 ï¿­ 3 ï¿­ 4");
}

TEST(TokenizerTest, PreserveSegmentAlphabetChange) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::SegmentAlphabetChange
                      | Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::PreserveSegmentedTokens);
  test_tok_and_detok(tokenizer, "rawĞ‘", "raw ï¿­ Ğ‘");
}

TEST(TokenizerTest, PreserveSegmentAlphabet) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::SegmentAlphabetChange
                      | Tokenizer::Flags::PreserveSegmentedTokens);
  tokenizer.add_alphabet_to_segment("Han");
  test_tok_and_detok(tokenizer, "æ¸¬è©¦abc", "æ¸¬ ï¿­ è©¦ ï¿­ abc");
}

TEST(TokenizerTest, PreserveSegmentCase) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::SegmentCase
                      | Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::PreserveSegmentedTokens);
  test_tok_and_detok(tokenizer, "WiFi", "Wi ï¿­ Fi");
}

TEST(TokenizerTest, PreserveSegmentCaseBPE) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      Tokenizer::Flags::SegmentCase
                      | Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::PreserveSegmentedTokens,
                      get_data("bpe-models/testcode.v0.1"));
  test_tok_and_detok(tokenizer, "iF", "i ï¿­ F");
}

TEST(TokenizerTest, BPEBasic) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative, Tokenizer::Flags::JoinerAnnotate,
                      get_data("bpe-models/testcode.v0.1"));
  test_tok_and_detok(tokenizer,
                     "abcdimprovementè”åˆå›½",
                     "aï¿­ bï¿­ cï¿­ dï¿­ imprï¿­ ovemenï¿­ tï¿­ è”åˆï¿­ å›½");
}

TEST(TokenizerTest, BPEModePrefix) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::None,
                      get_data("bpe-models/codes_prefix.fr"));
  test_tok(tokenizer,
           "Seulement seulement il vais nonseulement seulementnon Ã  Verdun",
           "S e u lement seulement il v ais n on se u lement seulement n on Ã  V er d un");
}

TEST(TokenizerTest, BPEModeNofix) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::None,
                      get_data("bpe-models/codes_nofix.fr"));
  test_tok(tokenizer,
           "Seulement seulement il vais nonseulement seulementnon Ã  Verdun",
           "S e u lement seulement il v ais n on seulement seulement n on Ã  V er d un");
}

TEST(TokenizerTest, BPEModeBoth) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::None,
                      get_data("bpe-models/codes_bothfix.fr"));
  test_tok(tokenizer,
           "Seulement seulement il vais nonseulement seulementnon Ã  Verdun",
           "S eu lement seulement il va is n on s eu lement seu l emen t n on Ã  V er du n");
}

TEST(TokenizerTest, BPECaseInsensitive) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::None,
                      get_data("bpe-models/codes_suffix_case_insensitive.fr"));
  test_tok(tokenizer,
           "Seulement seulement il vais nonseulement seulementnon Ã  Verdun",
           "Seulement seulement il va is n on seulement seu l em ent n on Ã  Ver d un");
}

TEST(TokenizerTest, BPEDropout) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative,
                      new BPE(get_data("bpe-models/codes_suffix_case_insensitive.fr"), 1.0));
  test_tok(tokenizer, "seulement", "s e u l e m e n t");
}

TEST(TokenizerTest, BPEVocabularyWithTrailingJoiner) {
  BPE* bpe = new BPE(get_data("bpe-models/bpe_code.v0.2"));
  bpe->set_vocabulary(std::vector<std::string>{"welï¿­"});
  Tokenizer tokenizer(Tokenizer::Mode::Space,
                      bpe,
                      Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::SupportPriorJoiners);
  test_tok(tokenizer, "welï¿­ le", "welï¿­ lï¿­ e");
  test_tok(tokenizer, "wel le", "wï¿­ eï¿­ l lï¿­ e");
}

TEST(TokenizerTest, BPEVocabularyWithLeadingJoiner) {
  BPE* bpe = new BPE(get_data("bpe-models/bpe_code.v0.2"));
  bpe->set_vocabulary(std::vector<std::string>{"ï¿­10"});
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      bpe,
                      Tokenizer::Flags::JoinerAnnotate);
  test_tok(tokenizer, "A10", "A ï¿­10");
  test_tok(tokenizer, "A100", "A ï¿­1ï¿­ 0ï¿­ 0");
}

TEST(TokenizerTest, BPEVocabularyWithSpacer) {
  Tokenizer::Options options;
  options.mode = Tokenizer::Mode::Space;
  options.spacer_annotate = true;

  auto bpe = std::make_shared<BPE>(get_data("bpe-models/bpe_code.v0.2"));
  bpe->set_vocabulary(std::vector<std::string>{"â–wel"}, &options);
  Tokenizer tokenizer(options, bpe);

  test_tok(tokenizer, "die welle", "d i e â–wel l e");
}

TEST(TokenizerTest, SpacerAnnotate) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::SpacerAnnotate);
  test_tok_and_detok(tokenizer,
                     "Isn't it so-greatly working?",
                     "Isn ' t â–it â–so - greatly â–working ?");
  test_tok_and_detok(tokenizer, "MP3", "MP 3");
}

TEST(TokenizerTest, SpacerNew) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      Tokenizer::Flags::SpacerAnnotate | Tokenizer::Flags::SpacerNew);
  test_tok_and_detok(tokenizer,
                     "Isn't it so-greatly working?",
                     "Isn ' t â– it â– so - greatly â– working ?");
}

TEST(TokenizerTest, Alphabets) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::SegmentAlphabetChange);
  std::unordered_map<std::string, size_t> lat_cyrillic_alphabets;
  lat_cyrillic_alphabets["Latin"] = 3;
  lat_cyrillic_alphabets["Cyrillic"] = 1;
  test_tok_alphabet(tokenizer, "rawĞ‘", "raw Ğ‘", lat_cyrillic_alphabets);

  std::unordered_map<std::string, size_t> han2;
  han2["Han"] = 2;
  test_tok_alphabet(tokenizer, "æœ‰ å…¥", "æœ‰ å…¥", han2);
}

TEST(TokenizerTest, ArabicAlphabet) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative);
  std::unordered_map<std::string, size_t> alphabets {
    {"Arabic", 5}
  };
  test_tok_alphabet(tokenizer, "Ù…Ø±Ø­Ø¨Ø§", "Ù…Ø±Ø­Ø¨Ø§", alphabets);
}

TEST(TokenizerTest, HalfWidthKanaAlphabet) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative);
  std::unordered_map<std::string, size_t> alphabets = {{"Katakana", 1}};
  test_tok_alphabet(tokenizer, "ï¾ƒ", "ï¾ƒ", alphabets);
}

TEST(TokenizerTest, NonbreakableSpace) {
  Tokenizer tokenizer(Tokenizer::Mode::Conservative);
  test_tok(tokenizer, "aÂ b", "a b");
}

TEST(TokenizerTest, CharMode) {
  Tokenizer tokenizer(Tokenizer::Mode::Char);
  test_tok(tokenizer, "  Hello   World 123.", "H e l l o W o r l d 1 2 3 .");
}

TEST(TokenizerTest, PriorJoinerSupportSpace) {
  Tokenizer tokenizer(Tokenizer::Mode::Space, Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::SupportPriorJoiners | Tokenizer::Flags::PreservePlaceholders);
  test_tok(tokenizer,
           "It is a test-aggressive ï¿­'ï¿­ with preï¿­ tokenizat ï¿­ions Worldï¿­ 123 and double ï¿­ï½Ÿmrk_placeï½ ï½Ÿmrk_holderï½ ï¿­ .",
           "It is a test-aggressive ï¿­'ï¿­ with preï¿­ tokenizat ï¿­ions Worldï¿­ 123 and double ï¿­ ï½Ÿmrk_placeï½  ï¿­ ï½Ÿmrk_holderï½  ï¿­ .");
}

TEST(TokenizerTest, NormalizeJoinerSpace) {
  Tokenizer tokenizer(Tokenizer::Mode::Space, Tokenizer::Flags::JoinerAnnotate);
  test_tok(tokenizer,
           "It is a test-aggressive â– 'â–  with preï¿­ tokenizat â– ions ï½Ÿentityï¼ƒ1ï¼šWorldï¿­ï½  123.",
           "It is a test-aggressive â– 'â–  with preâ–  tokenizat â– ions ï½Ÿentityï¼ƒ1ï¼šWorldï¿­ï½  123.");
}

TEST(TokenizerTest, PriorJoinerSupport) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::SupportPriorJoiners);
  test_tok(tokenizer,
           "It is a test-aggressive ï¿­'ï¿­ with preï¿­ tokenizat ï¿­ions Worldï¿­ 123.",
           "It is a test ï¿­-ï¿­ aggressive ï¿­'ï¿­ with preï¿­ tokenizat ï¿­ions Worldï¿­ 123 ï¿­.");
}

TEST(TokenizerTest, PriorJoinerAndPlaceholder) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::SupportPriorJoiners);
  test_tok(tokenizer, "ï½Ÿaï¿­bï½ ", "ï½Ÿaï¿­bï½ ");
}

TEST(TokenizerTest, CharModeSpacer) {
  Tokenizer tokenizer(Tokenizer::Mode::Char, Tokenizer::Flags::SpacerAnnotate);
  test_tok(tokenizer, "  Hello   World 123.", "H e l l o â–W o r l d â–1 2 3 .");
}

TEST(TokenizerTest, CharModeSpacerNew) {
  Tokenizer tokenizer(Tokenizer::Mode::Char, Tokenizer::Flags::SpacerAnnotate | Tokenizer::Flags::SpacerNew);
  test_tok(tokenizer, "  Hello   World 123.", "H e l l o â– W o r l d â– 1 2 3 .");
}

TEST(TokenizerTest, SentencePiece) {
  Tokenizer tokenizer(Tokenizer::Mode::None, Tokenizer::Flags::SentencePieceModel,
                      get_data("sp-models/sp.model"));
  test_tok_and_detok(tokenizer,
                     "The two shows, called Desire and Secrets, will be one-hour prime-time shows.",
                     "The â–two â–shows , â–called â–De si re â–and â–S e c re t s , â–will â–be â–one - hour â–prime - time â–shows .");
}

TEST(TokenizerTest, SentencePieceObject) {
  Tokenizer tokenizer(Tokenizer::Mode::None, new SentencePiece(get_data("sp-models/sp.model")));
  test_tok_and_detok(tokenizer,
                     "The two shows, called Desire and Secrets, will be one-hour prime-time shows.",
                     "The â–two â–shows , â–called â–De si re â–and â–S e c re t s , â–will â–be â–one - hour â–prime - time â–shows .");
}

TEST(TokenizerTest, SentencePieceWithJoinersAndPh) {
  Tokenizer tokenizer(Tokenizer::Mode::None,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::SentencePieceModel,
                      get_data("sp-models/sp.model"));
  test_tok_and_detok(tokenizer,
                     "The two shows, called ï½ŸDesireï½  and Secrets, will be one-hour prime-time shows.",
                     "The two shows ï¿­, called ï½ŸDesireï½  and S ï¿­e ï¿­c ï¿­re ï¿­t ï¿­s ï¿­, will be one ï¿­- ï¿­hour prime ï¿­- ï¿­time shows ï¿­.");
  test_tok_and_detok(tokenizer,
                     "The two shows, calledï½ŸDesireï½ and Secrets, will be one-hour prime-time shows.",
                     "The two shows ï¿­, calledï¿­ ï½ŸDesireï½ ï¿­ and S ï¿­e ï¿­c ï¿­re ï¿­t ï¿­s ï¿­, will be one ï¿­- ï¿­hour prime ï¿­- ï¿­time shows ï¿­.");
}

TEST(TokenizerTest, SentencePieceWithJoinersAndPh_preserve) {
  Tokenizer tokenizer(Tokenizer::Mode::None,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::SentencePieceModel|
                      Tokenizer::Flags::PreservePlaceholders,
                      get_data("sp-models/sp.model"));
  test_tok_and_detok(tokenizer,
                     "The two shows, calledï½ŸDesireï½ and Secrets, will be one-hour prime-time shows.",
                     "The two shows ï¿­, calledï¿­ ï½ŸDesireï½  ï¿­ and S ï¿­e ï¿­c ï¿­re ï¿­t ï¿­s ï¿­, will be one ï¿­- ï¿­hour prime ï¿­- ï¿­time shows ï¿­.");
}

TEST(TokenizerTest, SentencePieceSubwordRegularization) {
  Tokenizer tokenizer(get_data("sp-models/sp_regularization.model"), 1, 0.1);
  test_tok_and_detok(tokenizer,
                     "The two shows, called Desire and Secrets, will be one-hour prime-time shows.",
                     "â–The â– two â–show s , â–call ed â–De si re â– and â–Sec re t s , â–w ill â–be â–one - h our â– pri me - t im e â–show s .");
}

TEST(TokenizerTest, SentencePieceAlt) {
  Tokenizer tokenizer(Tokenizer::Mode::None, Tokenizer::Flags::SentencePieceModel,
                      get_data("sp-models/wmtende.model"));
  test_tok_and_detok(tokenizer,
                     "Bamford is appealing the sentence and has been granted bail of 50,000 baht.",
                     "â–Ba m ford â–is â–appealing â–the â–sentence â–and â–has â–been â–granted â–bail â–of â– 50,000 â–ba ht .");
}

TEST(TokenizerTest, SentencePieceWithJoiners) {
  Tokenizer tokenizer(Tokenizer::Mode::None,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::SentencePieceModel,
                      get_data("sp-models/sp.model"));
  test_tok_and_detok(tokenizer,
                     "The two shows, called Desire and Secrets, will be one-hour prime-time shows.",
                     "The two shows ï¿­, called De ï¿­si ï¿­re and S ï¿­e ï¿­c ï¿­re ï¿­t ï¿­s ï¿­, will be one ï¿­- ï¿­hour prime ï¿­- ï¿­time shows ï¿­.");
}

TEST(TokenizerTest, AggressiveWithSentencePiece) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::SentencePieceModel,
                      get_data("sp-models/wmtende.model"));
  test_tok(tokenizer,
           "Bamford is appealing the sentence and has been granted bail of 50,000 baht.",
           "Ba m ford is appealing the sentence and has been granted bail of 50 , 000 ba ht .");
}

TEST(TokenizerTest, AggressiveWithSentencePieceAndSpacers) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      Tokenizer::Flags::SpacerAnnotate | Tokenizer::Flags::SentencePieceModel,
                      get_data("sp-models/sp.model"));
  test_tok_and_detok(tokenizer,
                     "The two shows, called Desire and Secrets, will be one-hour prime-time shows.",
                     "The â–t wo â–s how s , â–called â–D es ir e â–and â–Se c re t s , â–will â–be â–one - hour â–p rime - time â–s how s .");
}

TEST(TokenizerTest, AggressiveWithSentencePieceAndJoiners) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::SentencePieceModel,
                      get_data("sp-models/sp.model"));
  test_tok_and_detok(tokenizer,
                     "The two shows, called Desire and Secrets, will be one-hour prime-time shows.",
                     "The t ï¿­wo s ï¿­how ï¿­s ï¿­, called D ï¿­es ï¿­ir ï¿­e and Se ï¿­c ï¿­re ï¿­t ï¿­s ï¿­, will be one ï¿­-ï¿­ hour p ï¿­rime ï¿­-ï¿­ time s ï¿­how ï¿­s ï¿­.");
}

TEST(TokenizerTest, SentencePieceIsolatedSpacer)
{
  Tokenizer tokenizer(Tokenizer::Mode::None,
                      Tokenizer::Flags::SentencePieceModel | Tokenizer::Flags::PreservePlaceholders,
                      get_data("sp-models/wmtende.model"));
  test_tok(tokenizer, "a crumpled sofa", "â–a â– cru mpl ed â–sofa");
  test_tok(tokenizer, "a ï½Ÿphï½ crumpled sofa", "â–a â– ï½Ÿphï½  cru mpl ed â–sofa");
}

TEST(TokenizerTest, SentencePieceIsolatedSpacerAndJoinerAnnotate)
{
  Tokenizer tokenizer(Tokenizer::Mode::None,
                      Tokenizer::Flags::SentencePieceModel
                      | Tokenizer::Flags::JoinerAnnotate
                      | Tokenizer::Flags::PreservePlaceholders,
                      get_data("sp-models/wmtende.model"));
  test_tok(tokenizer, "a crumpled sofa", "a cru ï¿­mpl ï¿­ed sofa");
  test_tok(tokenizer, "a ï½Ÿphï½ crumpled sofa", "a ï½Ÿphï½  ï¿­ cru ï¿­mpl ï¿­ed sofa");
  test_tok(tokenizer, "ï½Ÿphï½ ,", "ï½Ÿphï½  ï¿­ ,");
}

TEST(TokenizerTest, AggressiveWithSentencePieceIsolatedSpacerAndJoinerAnnotate) {
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      Tokenizer::Flags::SentencePieceModel | Tokenizer::Flags::JoinerAnnotate,
                      get_data("sp-models/wmtende.model"));
  test_tok(tokenizer, "depending on its temperature.", "depending on its temperature ï¿­.");
}

TEST(TokenizerTest, WithoutVocabulary) {
  Tokenizer tokenizer(Tokenizer::Mode::Space,
                      Tokenizer::Flags::JoinerAnnotate,
                      get_data("bpe-models/bpe_code.v0.2"),
                      "@@");
  test_tok(tokenizer,
           "Oliver GrÃ¼n , welle",
           "Oliver GrÃ¼n , welle");
}

TEST(TokenizerTest, WithVocabulary) {
  Tokenizer tokenizer(Tokenizer::Mode::Space,
                      Tokenizer::Flags::JoinerAnnotate,
                      get_data("bpe-models/bpe_code.v0.2"),
                      "@@",
                      get_data("bpe-models/vocab.en"),
                      50);
  test_tok(tokenizer,
           "Oliver GrÃ¼n , welle",
           "Oliver Gr@@ Ã¼@@ n , wel@@ le");
}

TEST(TokenizerTest, WithVocabularyTabSeparated) {
  Tokenizer tokenizer(Tokenizer::Mode::Space,
                      Tokenizer::Flags::JoinerAnnotate,
                      get_data("bpe-models/bpe_code.v0.2"),
                      "@@",
                      get_data("bpe-models/vocab.en.tab"),
                      50);
  test_tok(tokenizer,
           "Oliver GrÃ¼n , welle",
           "Oliver Gr@@ Ã¼@@ n , wel@@ le");
}

TEST(TokenizerTest, TokenInterface)
{
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive,
                      Tokenizer::Flags::JoinerAnnotate | Tokenizer::Flags::CaseMarkup);
  const std::string text = "Hello world!";
  std::vector<Token> tokens;
  tokenizer.tokenize(text, tokens);
  EXPECT_EQ(tokens[0].surface, "hello");
  EXPECT_EQ(tokens[1].surface, "world");
  EXPECT_EQ(tokens[2].surface, "!");
  EXPECT_EQ(tokenizer.detokenize(tokens), text);
}

TEST(TokenizerTest, DetokenizeEmptyToken)
{
  Tokenizer tokenizer(Tokenizer::Mode::Aggressive, Tokenizer::Flags::JoinerAnnotate);
  const std::vector<std::string> tokens = { "a", "", "b" };
  EXPECT_EQ(tokenizer.detokenize(tokens), "a b");
}

int main(int argc, char *argv[]) {
  testing::InitGoogleTest(&argc, argv);
  assert(argc == 2);
  data_dir = argv[1];
  return RUN_ALL_TESTS();
}
